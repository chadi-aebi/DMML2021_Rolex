{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Notebook_Rolex.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chadi-aebi/DMML2021_Rolex/blob/main/code/Notebook_Rolex.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ee1dPZ5x3h76"
      },
      "source": [
        "<h1> UNIL Team Rolex\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook we will proceed as follows:\n",
        "\n",
        "First we prepare the notebook by importing essential methods and components for text analytics. Then we will start with some preparations such as building a tokenizer with different possible features to apply this later in the classification.\n",
        "\n",
        "Subsequently, we start the text analytics divided by the different classifiers starting with a baseline calculation.\n",
        "Each classifier section starts with a classification without any data preprocessing or other features. Then we tune the hyperparameters for the classifier to find the best parameters. After that, we train models that also implement the preprocessing of data, we try word embeddings and in the end we try out dimensionality reduction and standardisation.\n",
        "\n",
        "The notebook has the following chapters:\n",
        "\n",
        "\n",
        "\n",
        "*   0.1.   Preparation to start working - impor necessary methods etc.\n",
        "*   0.2.   Further preparation for classification\n",
        "\n",
        "\n",
        "1.     Baseline calculation\n",
        "2.     Logistic Regression\n",
        "3.     kNN Classifier\n",
        "4.     Decision Tree\n",
        "5.     Random Forests\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wL9btvP1beMI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 0.1 Preparation to start working - import necessary methods etc."
      ],
      "metadata": {
        "id": "3TkfqWbwbecy"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owgx-k91Ov_g"
      },
      "source": [
        "**Remarks from Slack:** Basically we want to have your baseline solutions in that table. So without any data cleaning and pre-processing, who would the models mentioned in the table would perform (for each model you are also supposed to do hyper-parameter optimization to find the best hyper-parameters). This will give you the baseline accuracies that you can try to improve further by doing data preprocessing/cleaning or by using other models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gNRi80vlEyt1",
        "outputId": "ebd122ef-cb4e-4dd4-aff8-6b52f4066ef5"
      },
      "source": [
        "#Install and update spacy\n",
        "!pip install -U spacy\n",
        "#Download the french language model\n",
        "!python -m spacy download fr_core_news_sm"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (3.2.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.6)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.62.3)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.6.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.4.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.10.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (21.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.11.3)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in /usr/local/lib/python3.7/dist-packages (from spacy) (8.0.13)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.8.2)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.8.2)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (57.4.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.6)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy) (3.6.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy) (3.0.6)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy) (2.0.1)\n",
            "Collecting fr-core-news-sm==3.2.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_sm-3.2.0/fr_core_news_sm-3.2.0-py3-none-any.whl (17.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 17.4 MB 1.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.3.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from fr-core-news-sm==3.2.0) (3.2.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->fr-core-news-sm==3.2.0) (1.19.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->fr-core-news-sm==3.2.0) (2.0.6)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->fr-core-news-sm==3.2.0) (0.4.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->fr-core-news-sm==3.2.0) (0.8.2)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->fr-core-news-sm==3.2.0) (2.0.6)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->fr-core-news-sm==3.2.0) (1.0.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->fr-core-news-sm==3.2.0) (3.0.8)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->fr-core-news-sm==3.2.0) (57.4.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->fr-core-news-sm==3.2.0) (3.0.6)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->fr-core-news-sm==3.2.0) (0.4.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->fr-core-news-sm==3.2.0) (3.3.0)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->fr-core-news-sm==3.2.0) (2.4.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->fr-core-news-sm==3.2.0) (4.62.3)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->fr-core-news-sm==3.2.0) (8.0.13)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->fr-core-news-sm==3.2.0) (3.10.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->fr-core-news-sm==3.2.0) (2.23.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->fr-core-news-sm==3.2.0) (1.8.2)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->fr-core-news-sm==3.2.0) (0.6.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->fr-core-news-sm==3.2.0) (21.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->fr-core-news-sm==3.2.0) (2.11.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->fr-core-news-sm==3.2.0) (1.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.3.0,>=3.2.0->fr-core-news-sm==3.2.0) (3.6.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.3.0,>=3.2.0->fr-core-news-sm==3.2.0) (3.0.6)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.0->fr-core-news-sm==3.2.0) (5.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->fr-core-news-sm==3.2.0) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->fr-core-news-sm==3.2.0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->fr-core-news-sm==3.2.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->fr-core-news-sm==3.2.0) (1.24.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.3.0,>=3.2.0->fr-core-news-sm==3.2.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.3.0,>=3.2.0->fr-core-news-sm==3.2.0) (2.0.1)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('fr_core_news_sm')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sp-iRswNE44F"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import spacy\n",
        "import string\n",
        "import csv\n",
        "import time"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "leHXJLnvC3c2"
      },
      "source": [
        "#Classifier\n",
        "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
        "\n",
        "#Other\n",
        "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder, LabelEncoder, StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from spacy import displacy\n",
        "from spacy.lang.fr.stop_words import STOP_WORDS\n",
        "from spacy.lang.fr import French"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XBpIhxQR7iuC"
      },
      "source": [
        "# 0.2 Further preparations to starkt with classification\n",
        "\n",
        "Set random_seed, Vectorizers without preprocessing and load the french language model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pCDvQDBjHb2P"
      },
      "source": [
        "np.random_seed = 0"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FS9S8XIjQUa1"
      },
      "source": [
        "#Set TF-IDF and Count Vectorizer without any more specifications\n",
        "tfidf_vector = TfidfVectorizer()\n",
        "count_vector = CountVectorizer()\n",
        "#with preprocessing\n",
        "#tfidf_vector = TfidfVectorizer(tokenizer=spacy_tokenizer)\n"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzA0kkN1EKG2"
      },
      "source": [
        "#Load the french language model\n",
        "nlp = spacy.load('fr_core_news_sm')"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jiLVW789Q1HU"
      },
      "source": [
        "#Import stop words from french language model and puncutations\n",
        "stop_words=spacy.lang.fr.stop_words.STOP_WORDS\n",
        "punctuations = string.punctuation"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0efssDo2QJ-O"
      },
      "source": [
        "#Create a tokenizer function that can be used for preprocessing the data for classification - we try out different combinations of the sentence features\n",
        "\n",
        "def spacy_tokenizer(sentence):\n",
        "    # Create token object, which is used to create documents with linguistic annotations.\n",
        "    mytokens = nlp(sentence)\n",
        "\n",
        "    # Lemmatize each token and convert each token into lowercase\n",
        "    mytokens = [ word.lemma_.lower().strip() for word in mytokens ]\n",
        "    ## alternative way\n",
        "    # mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ]\n",
        "\n",
        "    # Remove stop words and punctuation\n",
        "    #mytokens = [ word for word in mytokens if word not in stop_words and word not in punctuations ]\n",
        "    # Remove punctuation\n",
        "    #mytokens = [ word for word in mytokens if word not in punctuations ]\n",
        "\n",
        "    # Remove anonymous dates and people\n",
        "    #mytokens = [ word.replace('xx/', '').replace('xxxx/', '').replace('xx', '') for word in mytokens ]\n",
        "    #mytokens = [ word for word in mytokens if word not in [\"xxxx\", \"xx\", \"\"]]\n",
        "\n",
        "    # Return preprocessed list of tokens\n",
        "    return mytokens"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We found that stopword removal did not lead to better results. This is well possible because by removing frequent and rather simple words you might remove the majority of words that appear in sentences of A1/A2 difficulty. Without those words it will be difficult to differentiate between sentences that are more sophisticated and those that only stay at a very basic level. "
      ],
      "metadata": {
        "id": "4w06MqoqeCRL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Function for model evaluation\n",
        "def evaluate(true, pred):\n",
        "    precision = precision_score(true, pred, average= 'macro')\n",
        "    recall = recall_score(true, pred, average = 'macro')\n",
        "    f1 = f1_score(true, pred, average = 'macro')\n",
        "    #print(f\"CONFUSION MATRIX:\\n{confusion_matrix(true, pred)}\")\n",
        "    print(f\"ACCURACY SCORE:\\n{accuracy_score(true, pred):.4f}\")\n",
        "    print(f\"CLASSIFICATION REPORT:\\n\\tPrecision: {precision:.4f}\\n\\tRecall: {recall:.4f}\\n\\tF1_Score: {f1:.4f}\")"
      ],
      "metadata": {
        "id": "jjRObxuTZwsB"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>  Getting started - text analytics per classifier"
      ],
      "metadata": {
        "id": "5HYObhzWi8Xa"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DbxvE-3a4b55"
      },
      "source": [
        "\n",
        "# 1. Baseline\n",
        "\n",
        "First, we start by calculating the baseline."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4vHFy3fGozP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45d496e6-5956-446f-a251-43d464b61026"
      },
      "source": [
        "data=pd.read_csv('https://raw.githubusercontent.com/chadi-aebi/DMML2021_Rolex/main/data/training_data.csv', index_col='id')\n",
        "X = data['sentence']\n",
        "ylabels = data['difficulty']\n",
        "print(ylabels.value_counts(normalize=True))"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A1    0.169375\n",
            "C2    0.168125\n",
            "C1    0.166250\n",
            "A2    0.165625\n",
            "B1    0.165625\n",
            "B2    0.165000\n",
            "Name: difficulty, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Logistic Regression\n",
        "<h2> 2.1 Logistic Regression without any data cleaning or tuning"
      ],
      "metadata": {
        "id": "7drQ-3Uzo2XT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We load the data..."
      ],
      "metadata": {
        "id": "pgAcP-acZ_cL"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ZA9m_yvB_vQ",
        "outputId": "2c75068d-a084-4361-b7ce-735f07916f97"
      },
      "source": [
        "lr_data=pd.read_csv('https://raw.githubusercontent.com/chadi-aebi/DMML2021_Rolex/main/data/training_data.csv', index_col='id')\n",
        "lr_test_df = pd.read_csv('https://raw.githubusercontent.com/chadi-aebi/DMML2021_Rolex/main/data/unlabelled_test_data.csv', index_col='id')\n",
        "lr_data.shape"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4800, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "..., select the features and train-test-split the data."
      ],
      "metadata": {
        "id": "LbLLdT9RaC75"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3tILch0iP2cj"
      },
      "source": [
        "X_lr = lr_data['sentence']\n",
        "ylabels_lr = lr_data['difficulty']\n",
        "\n",
        "X_train_lr, X_test_lr, y_train_lr, y_test_lr = train_test_split(X_lr, ylabels_lr, test_size=0.2, random_state=0, stratify=ylabels_lr)"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we define the classifier with its default settings..."
      ],
      "metadata": {
        "id": "fYfcwLCpam1c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define classifier\n",
        "lreg = LogisticRegression()"
      ],
      "metadata": {
        "id": "miLu_FfflSgQ"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "... and create the pipeline with the Classifier and the TF-IDF vectorizer without tokenization. Then we fit the model to our training data..."
      ],
      "metadata": {
        "id": "phD9QiDYan4-"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jf3Pp3f9P-Ue",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4dd3d9ed-6dea-435c-c4e1-0aac4a1869c3"
      },
      "source": [
        "# Create pipeline\n",
        "pipe = Pipeline([('vectorizer', tfidf_vector),\n",
        "                 ('classifier', lreg)])\n",
        "\n",
        "# Fit model on training set\n",
        "pipe.fit(X_train_lr, y_train_lr)"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(steps=[('vectorizer', TfidfVectorizer()),\n",
              "                ('classifier', LogisticRegression())])"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "... and make predicitions with this test data."
      ],
      "metadata": {
        "id": "td6YESpUa3_Z"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ggeYhJXwRtFU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4b78fde-5ffc-4e54-d5e9-6578c85ec4a4"
      },
      "source": [
        "# Predictions\n",
        "y_pred_lr = pipe.predict(X_test_lr)\n",
        "\n",
        "#accuracy_score(y_test_lr,y_pred_lr)\n",
        "evaluate(y_test_lr, y_pred_lr)\n"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ACCURACY SCORE:\n",
            "0.4604\n",
            "CLASSIFICATION REPORT:\n",
            "\tPrecision: 0.4578\n",
            "\tRecall: 0.4595\n",
            "\tF1_Score: 0.4554\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This was a first model without any other features. Let's have a look at some wrong predictions to find some hints what could be improved."
      ],
      "metadata": {
        "id": "nEgUC_jQrMJ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(X_test_lr, columns=[\"sentence\"])\n",
        "df[\"actual\"] = y_test_lr\n",
        "df[\"predicted\"] = y_pred_lr\n",
        "\n",
        "incorrect = df[df[\"actual\"] != df[\"predicted\"]]\n",
        "incorrect.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "NrpKDcxtrVLg",
        "outputId": "5e6059be-7e09-4647-dab2-351f55a14ecb"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>actual</th>\n",
              "      <th>predicted</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2003</th>\n",
              "      <td>Il est également connu pour ses publicités tél...</td>\n",
              "      <td>C1</td>\n",
              "      <td>B2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2585</th>\n",
              "      <td>Edgar, étincelant de furie, dominait tous les ...</td>\n",
              "      <td>C1</td>\n",
              "      <td>B2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2302</th>\n",
              "      <td>Ils sont heureux.</td>\n",
              "      <td>A1</td>\n",
              "      <td>B2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2958</th>\n",
              "      <td>Les canons renversèrent d'abord à peu près six...</td>\n",
              "      <td>C1</td>\n",
              "      <td>B2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3862</th>\n",
              "      <td>Parce que la philosophie se trouve de plus en ...</td>\n",
              "      <td>C1</td>\n",
              "      <td>C2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               sentence actual predicted\n",
              "id                                                                      \n",
              "2003  Il est également connu pour ses publicités tél...     C1        B2\n",
              "2585  Edgar, étincelant de furie, dominait tous les ...     C1        B2\n",
              "2302                                  Ils sont heureux.     A1        B2\n",
              "2958  Les canons renversèrent d'abord à peu près six...     C1        B2\n",
              "3862  Parce que la philosophie se trouve de plus en ...     C1        C2"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#submission_test_lr = pd.DataFrame(y_pred_lr, columns=['difficulty'])\n",
        "#submission_test_lr"
      ],
      "metadata": {
        "id": "nG_lAVrfgCLc"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#submission_test_lr.to_csv('submission_21-12-12.csv')"
      ],
      "metadata": {
        "id": "aXqVq5PWgHqd"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2> 2.2 Logistic Regression with hyperparameters tuning"
      ],
      "metadata": {
        "id": "mXbzubYRpAFR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section we searched for the best hyperparameters to improve our classifiers."
      ],
      "metadata": {
        "id": "Hheo85zWbAEE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define parameters to test\n",
        "\n",
        "#grid_lr = {\n",
        "#    'C': [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1, 10, 100],\n",
        "#    'penalty': ['none', 'l1', 'l2', 'elasticnet'],\n",
        "#    #'max_iter': list(range(100,800,100)),\n",
        "#    'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\n",
        "#}\n",
        "\n",
        "# Define and fit model\n",
        "\n",
        "#lreg = LogisticRegression()\n",
        "#lreg_cv = GridSearchCV(lreg, grid_lr, cv=10)\n",
        "#pipe = Pipeline([('vectorizer', tfidf_vector),\n",
        " #                ('classifier', lreg_cv)])\n",
        "\n",
        "#pipe.fit(X_train_lr, y_train_lr)\n",
        "\n",
        "# Print results\n",
        "\n",
        "#print(\"Hyperparameters:\", lreg_cv.best_params_)\n",
        "#y_pred_lr = pipe.predict(X_test_lr)\n",
        "#evaluate(y_test_lr, y_pred_lr)"
      ],
      "metadata": {
        "id": "weEZ4nUcAb2E"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We saved the hyperparameters and use them for the following predictions."
      ],
      "metadata": {
        "id": "3vvShqFYbGvN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#safe best parameters\n",
        "#Hyperparameters= {'C': 10, 'penalty': 'l2', 'solver': 'liblinear'}"
      ],
      "metadata": {
        "id": "n5qjNnn_OJYT"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2> 2.3 Logistic Regression with further methods "
      ],
      "metadata": {
        "id": "h5W6Q_50pLnr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3> 2.3.1 Preprocessing"
      ],
      "metadata": {
        "id": "F6Pcp8IkfdPg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section we extended the TF-IDF Vectorizer with the tokenizer we set at the beginning. By commenting some lines of the tokenizer we could try different settings and in the end keep the best combination."
      ],
      "metadata": {
        "id": "soqoTxPIcN3J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_vec_lr = TfidfVectorizer(tokenizer=spacy_tokenizer)"
      ],
      "metadata": {
        "id": "nN32JGDmNeOj"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define classifier with best hyperparameters\n",
        "lreg = LogisticRegression(C=10, penalty = 'l2', solver = 'liblinear')"
      ],
      "metadata": {
        "id": "-1Daf_BsYVlM"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create pipeline\n",
        "pipe = Pipeline([('vectorizer', tfidf_vec_lr),\n",
        "                 ('classifier', lreg)])\n",
        "\n",
        "# Fit model on training set\n",
        "pipe.fit(X_train_lr, y_train_lr)"
      ],
      "metadata": {
        "id": "dU4aRGsfNkMx",
        "outputId": "f62f5b5d-ca90-4252-a69c-4da6341796fd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 78,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Pipeline(steps=[('vectorizer',\n",
              "                 TfidfVectorizer(tokenizer=<function spacy_tokenizer at 0x7fa848a643b0>)),\n",
              "                ('classifier', LogisticRegression(C=10, solver='liblinear'))])"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Predictions\n",
        "y_pred_lr = pipe.predict(X_test_lr)\n",
        "\n",
        "#accuracy_score(y_test_lr,y_pred_lr)\n",
        "evaluate(y_test_lr, y_pred_lr)"
      ],
      "metadata": {
        "id": "Ccjxp0RgNn4t",
        "outputId": "ac706ecb-e36d-4254-9606-a32137a0e995",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ACCURACY SCORE:\n",
            "0.4938\n",
            "CLASSIFICATION REPORT:\n",
            "\tPrecision: 0.4899\n",
            "\tRecall: 0.4927\n",
            "\tF1_Score: 0.4899\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To make the best predictions for the kaggle data set, we take the parameters and settings from our best model and apply fit the model on the whole dataset."
      ],
      "metadata": {
        "id": "NR0416aXky8q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create pipeline\n",
        "pipe = Pipeline([('vectorizer', tfidf_vec_lr),\n",
        "                 ('classifier', lreg)])\n",
        "\n",
        "# Fit model on whole data set\n",
        "pipe.fit(X_lr, ylabels_lr)"
      ],
      "metadata": {
        "id": "xmlZ2E40N9b1",
        "outputId": "fbcf7347-3dc2-426e-a925-82d08f16dc8d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(steps=[('vectorizer',\n",
              "                 TfidfVectorizer(tokenizer=<function spacy_tokenizer at 0x7fa848a643b0>)),\n",
              "                ('classifier', LogisticRegression(C=10, solver='liblinear'))])"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_test=pipe.predict(lr_test_df['sentence'])"
      ],
      "metadata": {
        "id": "w3HeE12oOo1z"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to submit the predictions we save the predictions and create a csv file."
      ],
      "metadata": {
        "id": "JWP5FzcXlFaq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#submission_test = pd.DataFrame(y_pred_test, columns=['difficulty'])\n",
        "#submission_test"
      ],
      "metadata": {
        "id": "DPzps3KgOUnC"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#submission_test.to_csv('submission_21-12-15.csv')"
      ],
      "metadata": {
        "id": "_kbWTxmAOXrr"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3> 2.3.2 Word Embeddings"
      ],
      "metadata": {
        "id": "7RlGv4wtfbFB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this part, we use Word Embeddings instead of Tokenization. The code is modeled after the Kaggle course we saw in class. "
      ],
      "metadata": {
        "id": "UsODN9aockRv"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35a76cf2-1702-4220-a47d-54224e240896",
        "id": "qpc29nO2YLbt"
      },
      "source": [
        "#Vectorizing - Word Embeddings\n",
        "\n",
        "with nlp.disable_pipes():\n",
        "    doc_vectors = np.array([nlp(sentence).vector for sentence in lr_data.sentence])\n",
        "    \n",
        "doc_vectors.shape\n",
        "\n"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4800, 96)"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train_lr, X_test_lr, y_train_lr, y_test_lr = train_test_split(doc_vectors, lr_data.difficulty,\n",
        "                                                    test_size=0.2, random_state=0)\n",
        "\n"
      ],
      "metadata": {
        "id": "74FgTRZfYLbv"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit model on training set\n",
        "lreg.fit(X_train_lr, y_train_lr)"
      ],
      "metadata": {
        "outputId": "4e3e7e74-0518-42a2-8e9f-7d972365787c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s3q9HgldYLbw"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=10, solver='liblinear')"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_lr = lreg.predict(X_test_lr)\n",
        "\n",
        "evaluate(y_test_lr, y_pred_lr)"
      ],
      "metadata": {
        "outputId": "3db28569-6f4d-4815-aa15-e3971edcdbaa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FLhgqt1qYLby"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ACCURACY SCORE:\n",
            "0.3927\n",
            "CLASSIFICATION REPORT:\n",
            "\tPrecision: 0.3890\n",
            "\tRecall: 0.3938\n",
            "\tF1_Score: 0.3889\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3> 2.3.3 Dimensionality Reduction"
      ],
      "metadata": {
        "id": "hFKj5PD_fVXM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, we apply standardization and dimensionality reduction and the combination of both."
      ],
      "metadata": {
        "id": "ANWgSzNnjFif"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pca = PCA(n_components=900) #n_components can be varied to try out different models"
      ],
      "metadata": {
        "id": "q8dWBab9fZO3"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QhHZ3Oe4d6Ju"
      },
      "source": [
        "X_lr = lr_data['sentence']\n",
        "ylabels_lr = lr_data['difficulty']\n",
        "\n",
        "X_train_lr, X_test_lr, y_train_lr, y_test_lr = train_test_split(X_lr, ylabels_lr, test_size=0.2, random_state=0, stratify=ylabels_lr)"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_vec_lr = tfidf_vector.fit_transform(X_train_lr).toarray()\n",
        "X_test_vec_lr = tfidf_vector.transform(X_test_lr).toarray()\n",
        "print(X_train_vec_lr.shape)\n",
        "X_train_vec_lr"
      ],
      "metadata": {
        "id": "ATuNK3q1fwGX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b70ae0b2-ed05-497b-a96a-eacbe186da32"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3840, 12903)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#build pipe without Scaler & PCA\n",
        "scaler = StandardScaler()\n",
        "pipe = Pipeline([\n",
        "                 ('lreg', lreg),\n",
        "                 ])\n",
        "\n",
        "# Fit model\n",
        "pipe.fit(X_train_vec_lr, y_train_lr)\n",
        "print('Train Accuracy: ', round(pipe.score(X_train_vec_lr, y_train_lr), 4))\n",
        "print('Test Accuracy: ', round(pipe.score(X_test_vec_lr, y_test_lr), 4))"
      ],
      "metadata": {
        "id": "PeNEE4iIf6s-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c53870cf-320b-4b35-ff5e-0eb71de1d545"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy:  0.9859\n",
            "Test Accuracy:  0.4677\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#build pipe with  StandardScaler\n",
        "scaler = StandardScaler()\n",
        "pipe = Pipeline([\n",
        "                 ('scaler', scaler),\n",
        "                 ('lreg', lreg),\n",
        "                 ])\n",
        "\n",
        "# Fit model\n",
        "pipe.fit(X_train_vec_lr, y_train_lr)\n",
        "print('Train Accuracy: ', round(pipe.score(X_train_vec_lr, y_train_lr), 4))\n",
        "print('Test Accuracy: ', round(pipe.score(X_test_vec_lr, y_test_lr), 4))"
      ],
      "metadata": {
        "id": "Z6KEJ87CgMnE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7f54201-77ac-484c-f87b-3ef8e96b84f0"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy:  0.9992\n",
            "Test Accuracy:  0.4042\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#build pipe with PCA \n",
        "pipe = Pipeline([\n",
        "                 ('pca', pca),\n",
        "                 ('lreg', lreg),\n",
        "                 ])\n",
        "\n",
        "# Fit model\n",
        "\n",
        "pipe.fit(X_train_vec_lr, y_train_lr)\n",
        "print('Train Accuracy: ', round(pipe.score(X_train_vec_lr, y_train_lr), 4))\n",
        "print('Test Accuracy: ', round(pipe.score(X_test_vec_lr, y_test_lr), 4))"
      ],
      "metadata": {
        "id": "RmSXgfaogbvC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbaffbc1-57f7-41ea-eb81-403e2011d16d"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy:  0.7654\n",
            "Test Accuracy:  0.4354\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#build pipe with PCA & StandardScaler\n",
        "\n",
        "pipe = Pipeline([\n",
        "                 ('scaler', scaler),\n",
        "                 ('pca', pca),\n",
        "                 ('lreg', lreg),\n",
        "                 ])\n",
        "\n",
        "# Fit model\n",
        "\n",
        "pipe.fit(X_train_vec_lr, y_train_lr)\n",
        "print('Train Accuracy: ', round(pipe.score(X_train_vec_lr, y_train_lr), 4))\n",
        "print('Test Accuracy: ', round(pipe.score(X_test_vec_lr, y_test_lr), 4))"
      ],
      "metadata": {
        "id": "HE_xoFmFgn7J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53e6bb3c-b6b6-4d64-98fb-d5cd4776e2c3"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy:  0.7969\n",
            "Test Accuracy:  0.3938\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following parts are structured the same way but the classifier changes. The given explanation for the code is therefore more or less still valid for the following sections."
      ],
      "metadata": {
        "id": "t_D1CBQajNQY"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJp_X2jV6lP4"
      },
      "source": [
        "# 3. kNN\n",
        "<h2> 3.1 kNN without any data cleaning or tuning\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KmCSWQGs68Eq"
      },
      "source": [
        "knn_data=pd.read_csv('https://raw.githubusercontent.com/chadi-aebi/DMML2021_Rolex/main/data/training_data.csv', index_col='id')\n",
        "knn_test_df = pd.read_csv('https://raw.githubusercontent.com/chadi-aebi/DMML2021_Rolex/main/data/unlabelled_test_data.csv', index_col='id')"
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPjANU9g7LiI"
      },
      "source": [
        "X_knn = knn_data['sentence']\n",
        "ylabels_knn = knn_data['difficulty']\n",
        "\n",
        "X_train_knn, X_test_knn, y_train_knn, y_test_knn = train_test_split(X_knn, ylabels_knn, test_size=0.2, random_state=0, stratify=ylabels_knn)"
      ],
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define classifier\n",
        "knn = KNeighborsClassifier()\n"
      ],
      "metadata": {
        "id": "X65tlM6cl1UP"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ys-DfQ9Qg358",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1d8d521-efe8-4247-ef6f-2ed8319ca305"
      },
      "source": [
        "# Create pipeline\n",
        "pipe = Pipeline([('vectorizer', tfidf_vector),\n",
        "                 ('classifier', knn)])\n",
        "\n",
        "# Fit model on training set\n",
        "pipe.fit(X_train_knn, y_train_knn)"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(steps=[('vectorizer', TfidfVectorizer()),\n",
              "                ('classifier', KNeighborsClassifier())])"
            ]
          },
          "metadata": {},
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPglJHd8gsVc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dffa34ac-4829-4f87-9ef8-0b920c4da6ed"
      },
      "source": [
        "y_pred_knn = pipe.predict(X_test_knn)\n",
        "\n",
        "evaluate(y_test_knn, y_pred_knn)"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ACCURACY SCORE:\n",
            "0.3156\n",
            "CLASSIFICATION REPORT:\n",
            "\tPrecision: 0.3823\n",
            "\tRecall: 0.3137\n",
            "\tF1_Score: 0.2913\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2> 3.2 kNN with hyperparameter tuning\n"
      ],
      "metadata": {
        "id": "E9TvPasnpxAF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define parameters to test\n",
        "\n",
        "#grid_knn = {'n_neighbors':np.arange(1,100),\n",
        " #       'p':np.arange(1,3),\n",
        " #       'weights':['uniform','distance']\n",
        " #      }\n",
        "\n",
        "# Define and fit model\n",
        "\n",
        "#knn = KNeighborsClassifier()\n",
        "#knn_cv = GridSearchCV(knn, grid, cv=10)\n",
        "#pipe = Pipeline([('vectorizer', tfidf_vector),\n",
        " #                ('classifier', knn_cv)])\n",
        "\n",
        "#pipe.fit(X_train_knn, y_train_knn)\n",
        "\n",
        "# Print results\n",
        "\n",
        "#print(\"Hyperparameters:\", knn_cv.best_params_)\n",
        "#y_pred_knn = pipe.predict(X_test_knn)\n",
        "#evaluate(y_test_knn, y_pred_knn)"
      ],
      "metadata": {
        "id": "4XN_daqtn_36"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#bestparams_knn = {'n_neighbors': 82, 'p': 2, 'weights': 'distance'}"
      ],
      "metadata": {
        "id": "VqiIDe_tuXZV"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2> 3.3 kNN with further methods"
      ],
      "metadata": {
        "id": "iMRoTJgxp2vZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3> 3.3.1 Preprocessing"
      ],
      "metadata": {
        "id": "Lwa2zJYUli44"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Define classifier with best parameters\n",
        "knn = KNeighborsClassifier(n_neighbors= 82, weights= 'distance')"
      ],
      "metadata": {
        "id": "dncB-Z4vXl7c"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3> 3.3.2 Word Embeddings"
      ],
      "metadata": {
        "id": "e07_-cDOlrUq"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5cad1647-31ac-453b-871c-f0bdf380fb31",
        "id": "soVJLmmEXX81"
      },
      "source": [
        "#Vectorizing - Word Embeddings\n",
        "\n",
        "with nlp.disable_pipes():\n",
        "    doc_vectors = np.array([nlp(sentence).vector for sentence in knn_data.sentence])\n",
        "    \n",
        "doc_vectors.shape\n",
        "\n"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4800, 96)"
            ]
          },
          "metadata": {},
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train_knn, X_test_knn, y_train_knn, y_test_knn = train_test_split(doc_vectors, knn_data.difficulty,\n",
        "                                                    test_size=0.2, random_state=0)\n",
        "\n"
      ],
      "metadata": {
        "id": "8tU4aFhxXX85"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit model on training set\n",
        "knn.fit(X_train_knn, y_train_knn)"
      ],
      "metadata": {
        "outputId": "3bb8f582-5b90-4972-bd2e-47cd7bfa8228",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MlQtb4MhXX87"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "KNeighborsClassifier(n_neighbors=82, weights='distance')"
            ]
          },
          "metadata": {},
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_knn = knn.predict(X_test_knn)\n",
        "\n",
        "evaluate(y_test_knn, y_pred_knn)"
      ],
      "metadata": {
        "outputId": "30b77869-3530-4c16-8b6d-6f72f8710488",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pKJNKWXGXX89"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ACCURACY SCORE:\n",
            "0.2615\n",
            "CLASSIFICATION REPORT:\n",
            "\tPrecision: 0.3307\n",
            "\tRecall: 0.2588\n",
            "\tF1_Score: 0.2252\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3> 3.3.3 Dimensionality Reduction"
      ],
      "metadata": {
        "id": "JC2JGEHNlum0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pca = PCA(n_components=200) #n_components can be varied to try out different models"
      ],
      "metadata": {
        "id": "yKiXFBeAmF7F"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DiAb3k01d_JM"
      },
      "source": [
        "X_knn = knn_data['sentence']\n",
        "ylabels_knn = knn_data['difficulty']\n",
        "\n",
        "X_train_knn, X_test_knn, y_train_knn, y_test_knn = train_test_split(X_knn, ylabels_knn, test_size=0.2, random_state=0, stratify=ylabels_knn)"
      ],
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_vec_knn = tfidf_vector.fit_transform(X_train_knn).toarray()\n",
        "X_test_vec_knn = tfidf_vector.transform(X_test_knn).toarray()\n",
        "print(X_train_vec_knn.shape)\n",
        "X_train_vec_knn"
      ],
      "metadata": {
        "outputId": "318e96f1-c36f-4964-fffd-50e47c6dc56e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LbN0__1kmF7G"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3840, 12903)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#build pipe without Scaler & PCA\n",
        "scaler = StandardScaler()\n",
        "pipe = Pipeline([\n",
        "                 ('knn', knn),\n",
        "                 ])\n",
        "\n",
        "# Fit model\n",
        "pipe.fit(X_train_vec_knn, y_train_knn)\n",
        "print('Train Accuracy: ', round(pipe.score(X_train_vec_knn, y_train_knn), 4))\n",
        "print('Test Accuracy: ', round(pipe.score(X_test_vec_knn, y_test_knn), 4))"
      ],
      "metadata": {
        "outputId": "4e2d8e03-e2ca-47f9-ca6d-443cc430d7f7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g4Z_UnJTlyQm"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy:  0.9992\n",
            "Test Accuracy:  0.3354\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#build pipe with  StandardScaler\n",
        "scaler = StandardScaler()\n",
        "pipe = Pipeline([\n",
        "                 ('scaler', scaler),\n",
        "                 ('knn', knn),\n",
        "                 ])\n",
        "\n",
        "# Fit model\n",
        "pipe.fit(X_train_vec_knn, y_train_knn)\n",
        "print('Train Accuracy: ', round(pipe.score(X_train_vec_knn, y_train_knn), 4))\n",
        "print('Test Accuracy: ', round(pipe.score(X_test_vec_knn, y_test_knn), 4))"
      ],
      "metadata": {
        "id": "1bwqJb6jlyQo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a766ed9-8855-4ba4-edcb-1d5b482e9176"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy:  0.9992\n",
            "Test Accuracy:  0.1719\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#build pipe with PCA \n",
        "pipe = Pipeline([\n",
        "                 ('pca', pca),\n",
        "                 ('knn', knn),\n",
        "                 ])\n",
        "\n",
        "\n",
        "# Fit model\n",
        "pipe.fit(X_train_vec_knn, y_train_knn)\n",
        "print('Train Accuracy: ', round(pipe.score(X_train_vec_knn, y_train_knn), 4))\n",
        "print('Test Accuracy: ', round(pipe.score(X_test_vec_knn, y_test_knn), 4))"
      ],
      "metadata": {
        "id": "L1CW1RzxlyQq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12c96668-acfa-4322-a91d-015a219fd268"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy:  0.9992\n",
            "Test Accuracy:  0.2427\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#build pipe with PCA & StandardScaler\n",
        "\n",
        "pipe = Pipeline([\n",
        "                 ('scaler', scaler),\n",
        "                 ('pca', pca),\n",
        "                 ('knn', knn),\n",
        "                 ])\n",
        "\n",
        "# Fit model\n",
        "pipe.fit(X_train_vec_knn, y_train_knn)\n",
        "print('Train Accuracy: ', round(pipe.score(X_train_vec_knn, y_train_knn), 4))\n",
        "print('Test Accuracy: ', round(pipe.score(X_test_vec_knn, y_test_knn), 4))"
      ],
      "metadata": {
        "id": "bKQ4lwWqlyQs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa3df04e-e24d-467b-85ee-95de928d597c"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy:  0.9987\n",
            "Test Accuracy:  0.1979\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYol3GFa6wzs"
      },
      "source": [
        "# 4. Decision Tree\n",
        "<h2> 4.1 Decision Tree without any data cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7VcEoDF-6_rZ"
      },
      "source": [
        "tree_data=pd.read_csv('https://raw.githubusercontent.com/chadi-aebi/DMML2021_Rolex/main/data/training_data.csv', index_col='id')\n",
        "tree_test_df = pd.read_csv('https://raw.githubusercontent.com/chadi-aebi/DMML2021_Rolex/main/data/unlabelled_test_data.csv', index_col='id')"
      ],
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oSPymGOk7PfG"
      },
      "source": [
        "X_tree = tree_data['sentence']\n",
        "ylabels_tree = tree_data['difficulty']\n",
        "\n",
        "X_train_tree, X_test_tree, y_train_tree, y_test_tree = train_test_split(X_tree, ylabels_tree, test_size=0.2, random_state=0, stratify=ylabels_tree)"
      ],
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define classifier\n",
        "tree = DecisionTreeClassifier()"
      ],
      "metadata": {
        "id": "zWfElWrvm15t"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8sFIl3VHjWZk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64c443c2-75bc-4ea9-8245-f0fb95b0474e"
      },
      "source": [
        "\n",
        "# Create pipeline\n",
        "pipe = Pipeline([('vectorizer', tfidf_vector),\n",
        "                 ('classifier', tree)])\n",
        "\n",
        "# Fit model on training set\n",
        "pipe.fit(X_train_tree, y_train_tree)"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(steps=[('vectorizer', TfidfVectorizer()),\n",
              "                ('classifier', DecisionTreeClassifier())])"
            ]
          },
          "metadata": {},
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGPV-iKDifr_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a183b89-7456-4041-e310-05d9e78cb524"
      },
      "source": [
        "y_pred_tree = pipe.predict(X_test_tree)\n",
        "\n",
        "accuracy_score(y_test_tree,y_pred_tree)\n",
        "#evaluate(y_test_tree, y_pred_tree)"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.3"
            ]
          },
          "metadata": {},
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2> 4.2 Decision Tree with hyperparameter tuning"
      ],
      "metadata": {
        "id": "B53JCf7GqIHD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Grid Search - hyperparameter tuning\n",
        "\n",
        "\n",
        "# Define parameters to test\n",
        "#grid_tree ={\"criterion\" : [\"gini\", \"entropy\"],\n",
        "#            \"splitter\":[\"best\",\"random\"],\n",
        "#            \"max_depth\" : [1,5,20,50,100, None],\n",
        "#           \"min_samples_leaf\":[1,5,20,50,100, None],\n",
        "#           \"max_features\":[\"auto\",\"log2\",\"sqrt\",None],\n",
        "#           \"max_leaf_nodes\":[None,10,50,100] }\n",
        "\n",
        "# Define and fit model\n",
        "#tree = tree = DecisionTreeClassifier()\n",
        "#tree_cv = GridSearchCV(tree, grid_tree, cv=10)\n",
        "#pipe = Pipeline([('vectorizer', tfidf_vector),\n",
        "#                 ('classifier', tree_cv)])\n",
        "\n",
        "#pipe.fit(X_train_tree, y_train_tree)\n",
        "\n",
        "# Print results\n",
        "#print(\"Hyperparameters:\", tree_cv.best_params_)\n",
        "#y_pred_tree = pipe.predict(X_test_tree)\n",
        "#evaluate(y_test_tree, y_pred_tree)"
      ],
      "metadata": {
        "id": "vWKVW9MsqCOB"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We save the best parameters from the GridSearch to use it for further models:"
      ],
      "metadata": {
        "id": "_f2kKxaNx83u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#bestparams_tree = {'criterion': 'gini', 'max_depth': 20, 'max_features': None, 'max_leaf_nodes': 100, 'min_samples_leaf': 1, 'splitter': 'best'}"
      ],
      "metadata": {
        "id": "37yCmPd-wVhf"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2> 4.3 Decision Tree with further methods"
      ],
      "metadata": {
        "id": "Z4cvQtWaqMgs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3> 4.3.1 Preprocessing\n"
      ],
      "metadata": {
        "id": "w5BmhwQTme4g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_vec_tree = TfidfVectorizer(tokenizer=spacy_tokenizer)"
      ],
      "metadata": {
        "id": "szhY96mrxGea"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define classifier with best params\n",
        "tree = DecisionTreeClassifier(criterion = 'gini', max_depth = 20, max_features = None, max_leaf_nodes= 100, min_samples_leaf = 1, splitter = 'best')\n"
      ],
      "metadata": {
        "id": "E32C0uiowi8X"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create pipeline\n",
        "pipe = Pipeline([('vectorizer', tfidf_vec_tree),\n",
        "                 ('classifier', tree)])\n",
        "\n",
        "# Fit model on training set\n",
        "pipe.fit(X_train_tree, y_train_tree)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lOh9FXOHXGv7",
        "outputId": "1743d5a7-95e2-465c-c0f6-66c1d93381bf"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(steps=[('vectorizer',\n",
              "                 TfidfVectorizer(tokenizer=<function spacy_tokenizer at 0x7fa848a643b0>)),\n",
              "                ('classifier',\n",
              "                 DecisionTreeClassifier(max_depth=20, max_leaf_nodes=100))])"
            ]
          },
          "metadata": {},
          "execution_count": 123
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Predictions\n",
        "y_pred_tree = pipe.predict(X_test_tree)\n",
        "\n",
        "evaluate(y_test_tree, y_pred_tree)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WqJwKV1Hxv8t",
        "outputId": "44b3254c-31bd-407e-8d97-3f4410c47d9e"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ACCURACY SCORE:\n",
            "0.3281\n",
            "CLASSIFICATION REPORT:\n",
            "\tPrecision: 0.3392\n",
            "\tRecall: 0.3273\n",
            "\tF1_Score: 0.3224\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3> 4.3.2 Word embedding\n"
      ],
      "metadata": {
        "id": "eT2DTFqDmjH1"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5093d6b6-b52c-4d89-f5f8-df4ffa964da9",
        "id": "csRdtKxtW74V"
      },
      "source": [
        "#Vectorizing - Word Embeddings\n",
        "\n",
        "with nlp.disable_pipes():\n",
        "    doc_vectors = np.array([nlp(sentence).vector for sentence in tree_data.sentence])\n",
        "    \n",
        "doc_vectors.shape\n",
        "\n"
      ],
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4800, 96)"
            ]
          },
          "metadata": {},
          "execution_count": 125
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train_tree, X_test_tree, y_train_tree, y_test_tree = train_test_split(doc_vectors, tree_data.difficulty,\n",
        "                                                    test_size=0.2, random_state=0)\n",
        "\n"
      ],
      "metadata": {
        "id": "m6YwUAs6W74X"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit model on training set\n",
        "tree.fit(X_train_tree, y_train_tree)"
      ],
      "metadata": {
        "outputId": "442082ce-d195-4a71-8cd8-0b2437c0b71c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DJm5OuCpW74Y"
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DecisionTreeClassifier(max_depth=20, max_leaf_nodes=100)"
            ]
          },
          "metadata": {},
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_tree = tree.predict(X_test_tree)\n",
        "\n",
        "evaluate(y_test_tree, y_pred_tree)"
      ],
      "metadata": {
        "outputId": "026bd426-fe85-4d31-dfca-13b69098503e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Odfi-C-EW74a"
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ACCURACY SCORE:\n",
            "0.3104\n",
            "CLASSIFICATION REPORT:\n",
            "\tPrecision: 0.3122\n",
            "\tRecall: 0.3097\n",
            "\tF1_Score: 0.3085\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3> 4.3.3 Dimensionality Reduction"
      ],
      "metadata": {
        "id": "fqXWyJ4Dmp7q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pca = PCA(n_components=200) #n_components can be varied to try out different models"
      ],
      "metadata": {
        "id": "SI6deZ4ImzJD"
      },
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8lvLlLvYeDUF"
      },
      "source": [
        "X_tree = tree_data['sentence']\n",
        "ylabels_tree = tree_data['difficulty']\n",
        "\n",
        "X_train_tree, X_test_tree, y_train_tree, y_test_tree = train_test_split(X_tree, ylabels_tree, test_size=0.2, random_state=0, stratify=ylabels_tree)"
      ],
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_vec_tree = tfidf_vector.fit_transform(X_train_tree).toarray()\n",
        "X_test_vec_tree = tfidf_vector.transform(X_test_tree).toarray()\n",
        "print(X_train_vec_tree.shape)\n",
        "X_train_vec_tree"
      ],
      "metadata": {
        "outputId": "76864e25-3490-45a7-aaeb-7d48ef170e15",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xzXxgI0gmzJG"
      },
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3840, 12903)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 131
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#build pipe without Scaler & PCA\n",
        "scaler = StandardScaler()\n",
        "pipe = Pipeline([\n",
        "                 ('tree', tree),\n",
        "                 ])\n",
        "\n",
        "# Fit model\n",
        "pipe.fit(X_train_vec_tree, y_train_tree)\n",
        "print('Train Accuracy: ', round(pipe.score(X_train_vec_tree, y_train_tree), 4))\n",
        "print('Test Accuracy: ', round(pipe.score(X_test_vec_tree, y_test_tree), 4))"
      ],
      "metadata": {
        "outputId": "72978cca-23da-4d55-e384-4fc05907e9c5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FQWNeBZtmzJI"
      },
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy:  0.4612\n",
            "Test Accuracy:  0.3281\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#build pipe with  StandardScaler\n",
        "scaler = StandardScaler()\n",
        "pipe = Pipeline([\n",
        "                 ('scaler', scaler),\n",
        "                 ('tree', tree),\n",
        "                 ])\n",
        "# Fit model\n",
        "pipe.fit(X_train_vec_tree, y_train_tree)\n",
        "print('Train Accuracy: ', round(pipe.score(X_train_vec_tree, y_train_tree), 4))\n",
        "print('Test Accuracy: ', round(pipe.score(X_test_vec_tree, y_test_tree), 4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63lmb9XGmzJK",
        "outputId": "ba8f4801-e8b5-43a2-b2e3-c3653bb84fef"
      },
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy:  0.4612\n",
            "Test Accuracy:  0.3281\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#build pipe with PCA \n",
        "pipe = Pipeline([\n",
        "                 ('pca', pca),\n",
        "                 ('tree', tree),\n",
        "                 ])\n",
        "\n",
        "\n",
        "# Fit model\n",
        "pipe.fit(X_train_vec_tree, y_train_tree)\n",
        "print('Train Accuracy: ', round(pipe.score(X_train_vec_tree, y_train_tree), 4))\n",
        "print('Test Accuracy: ', round(pipe.score(X_test_vec_tree, y_test_tree), 4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xlx1jX9kmzJK",
        "outputId": "5c2185e3-5b07-4e2f-e8b1-c218401b1d51"
      },
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy:  0.5055\n",
            "Test Accuracy:  0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#build pipe with PCA & StandardScaler\n",
        "\n",
        "pipe = Pipeline([\n",
        "                 ('scaler', scaler),\n",
        "                 ('pca', pca),\n",
        "                 ('tree', tree),\n",
        "                 ])\n",
        "\n",
        "# Fit model\n",
        "pipe.fit(X_train_vec_tree, y_train_tree)\n",
        "print('Train Accuracy: ', round(pipe.score(X_train_vec_tree, y_train_tree), 4))\n",
        "print('Test Accuracy: ', round(pipe.score(X_test_vec_tree, y_test_tree), 4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XX8dynk5mzJL",
        "outputId": "36ad0f72-a537-4739-9caf-b3edef5bc02c"
      },
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy:  0.5495\n",
            "Test Accuracy:  0.3073\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zg-NDltT62B0"
      },
      "source": [
        "# 5. Random Forest\n",
        "<h2> 5.1 Random Forest without any data cleaning\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_IMuTls7C1j"
      },
      "source": [
        "rf_data=pd.read_csv('https://raw.githubusercontent.com/chadi-aebi/DMML2021_Rolex/main/data/training_data.csv', index_col='id')\n",
        "rf_test_df = pd.read_csv('https://raw.githubusercontent.com/chadi-aebi/DMML2021_Rolex/main/data/unlabelled_test_data.csv', index_col='id')\n",
        "#rf_test_df.head()"
      ],
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QkFia3ER7Ygu"
      },
      "source": [
        "X_rf = rf_data['sentence']\n",
        "ylabels_rf = rf_data['difficulty']\n",
        "\n",
        "X_train_rf, X_test_rf, y_train_rf, y_test_rf = train_test_split(X_rf, ylabels_rf, test_size=0.2, random_state=0, stratify=ylabels_rf)"
      ],
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_rf_df = pd.DataFrame(X_train_rf)"
      ],
      "metadata": {
        "id": "dka8IrRm6dar"
      },
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0MTWwJkhh5p"
      },
      "source": [
        "# Define classifier\n",
        "rfc = RandomForestClassifier()"
      ],
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6c3bxy72SC3d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22ff9a98-95ca-49db-dadb-dceb95404f38"
      },
      "source": [
        "# Create pipeline with tfidf\n",
        "pipe = Pipeline([('vectorizer', tfidf_vector),\n",
        "                 ('classifier', rfc)])\n",
        "\n",
        "# Fit model on training set\n",
        "pipe.fit(X_train_rf, y_train_rf)"
      ],
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(steps=[('vectorizer', TfidfVectorizer()),\n",
              "                ('classifier', RandomForestClassifier())])"
            ]
          },
          "metadata": {},
          "execution_count": 140
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_rf = pipe.predict(X_test_rf)\n",
        "\n",
        "#accuracy_score(y_test_rf,y_pred_rf)\n",
        "evaluate(y_test_rf, y_pred_rf)"
      ],
      "metadata": {
        "id": "6J2i2Fppq4L-",
        "outputId": "3618e689-8ae0-468d-b216-472668bbb40b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ACCURACY SCORE:\n",
            "0.3927\n",
            "CLASSIFICATION REPORT:\n",
            "\tPrecision: 0.3887\n",
            "\tRecall: 0.3912\n",
            "\tF1_Score: 0.3777\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2> 5.2 Random Forest with Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "S8l4EDfrqcez"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In contrast to the other classifiers, we also tried RandomizedSearchCV for the Hyperparameter Tuning in this case. "
      ],
      "metadata": {
        "id": "ixe8BPL2b9hN"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uG3X64vahNoI"
      },
      "source": [
        "#Tuning Hyperparameters with RandomizedSearchCV\n",
        "\n",
        "# Number of trees in random forest\n",
        "#n_estimators = [int(x) for x in np.linspace(start = 500, stop = 2000, num = 10)]\n",
        "\n",
        "# Number of features to consider at every split\n",
        "#max_features = ['auto', 'sqrt']\n",
        "\n",
        "# Maximum number of levels in tree\n",
        "#max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
        "#max_depth.append(None)\n",
        "\n",
        "# Minimum number of samples required to split a node\n",
        "#min_samples_split = [2, 5, 10]\n",
        "\n",
        "# Minimum number of samples required at each leaf node\n",
        "#min_samples_leaf = [1, 2, 4]\n",
        "\n",
        "# Method of selecting samples for training each tree\n",
        "#bootstrap = [True, False]\n",
        "\n",
        "# Create the grid\n",
        "#grid_rf = {'n_estimators': n_estimators,\n",
        "#               'max_features': max_features,\n",
        "#               'max_depth': max_depth,\n",
        "#               'min_samples_split': min_samples_split,\n",
        "#               'min_samples_leaf': min_samples_leaf,\n",
        "#               'bootstrap': bootstrap}\n",
        "\n",
        "#print(random_grid)"
      ],
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UmbrlJcrhU8Y"
      },
      "source": [
        "#Crossvalidation with RandomizedSearchCV\n",
        "#rf_random = RandomizedSearchCV(estimator = rfc, param_distributions = random_grid_rf, n_iter = 10, cv = 5, verbose=2, random_state=0, n_jobs = -1)\n",
        "\n",
        "# Create pipeline with tfidf\n",
        "#pipe = Pipeline([('vectorizer', tfidf_vector),\n",
        " #                ('classifier', rf_random)])\n",
        "\n",
        "# Fit model on training set\n",
        "#pipe.fit(X_train_rf, y_train_rf)"
      ],
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kipbly2yiLDe"
      },
      "source": [
        "#Save and display best parameters\n",
        "#best_param_1 = (bootstrap=False,\n",
        "# max_depth= 70,\n",
        "# max_features= 'auto',\n",
        "# min_samples_leaf= 1,\n",
        "# min_samples_split= 10,\n",
        "# n_estimators= 1166)"
      ],
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define and fit model with GridSearchCV\n",
        "\n",
        "#rfc = RandomForestClassifier()\n",
        "#rfc_cv = GridSearchCV(rfc, grid_rfc, cv=10)\n",
        "#pipe = Pipeline([('vectorizer', tfidf_vector),\n",
        "#                 ('classifier', rfc_cv)])\n",
        "\n",
        "#pipe.fit(X_train_rf, y_train_rf)\n",
        "\n",
        "# Print results\n",
        "\n",
        "#print(\"Hyperparameters:\", rfc_cv.best_params_)\n",
        "#y_pred_tree = pipe.predict(X_test_rf)\n",
        "#evaluate(y_test_tree, y_pred_rf)"
      ],
      "metadata": {
        "id": "0zxK-sdMBHeA"
      },
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2> 5.3 Random Forest with further methods"
      ],
      "metadata": {
        "id": "D_e5qIOAqq9b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3> 5.3.1 Preprocessing"
      ],
      "metadata": {
        "id": "kmVABUvzrOyX"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gwkMbWKPlyiY"
      },
      "source": [
        "tfidf_vec_rf = TfidfVectorizer(tokenizer=spacy_tokenizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define classifier with best params - 07.12.2021 #2\n",
        "rfc = RandomForestClassifier(bootstrap=False,\n",
        " max_depth= 70,\n",
        " max_features= 'auto',\n",
        " min_samples_leaf= 1,\n",
        " min_samples_split= 10,\n",
        " n_estimators= 1166)"
      ],
      "metadata": {
        "id": "pP08OpFIOIoX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create pipeline with tfidf (Use whole dataset)\n",
        "pipe = Pipeline([('vectorizer', tfidf_vec_rf),\n",
        "                 ('classifier', rfc)])\n",
        "\n",
        "# Fit model on whole set\n",
        "pipe.fit(X_rf, ylabels_rf)"
      ],
      "metadata": {
        "id": "WSlZFnhxIOqJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create pipeline with tfidf\n",
        "pipe = Pipeline([('vectorizer', tfidf_vec_rf),\n",
        "                 ('classifier', rfc)])\n",
        "\n",
        "# Fit model on training set\n",
        "pipe.fit(X_train_rf, y_train_rf)"
      ],
      "metadata": {
        "id": "zQPTFI9lzjwr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I1tDTARLjbnd"
      },
      "source": [
        "y_pred_rf = pipe.predict(X_test_rf)\n",
        "\n",
        "#accuracy_score(y_test_rf,y_pred_rf)\n",
        "evaluate(y_test_rf, y_pred_rf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFIi0cuR_0Bh"
      },
      "source": [
        "#y_pred_test=pipe.predict(rf_test_df['sentence'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wv3TWRkWC02p"
      },
      "source": [
        "#submission_test = pd.DataFrame(y_pred_test, columns=['difficulty'])\n",
        "#submission_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aFMazOxqCbAO"
      },
      "source": [
        "\n",
        "#submission_test.to_csv('submission_21-12-13_2.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3> 5.3.2 Word Embeddings"
      ],
      "metadata": {
        "id": "6VQWY5NrrHFI"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bIrshLbcI1mk"
      },
      "source": [
        "#Vectorizing - Word Embeddings\n",
        "\n",
        "with nlp.disable_pipes():\n",
        "    doc_vectors = np.array([nlp(sentence).vector for sentence in rf_data.sentence])\n",
        "    \n",
        "doc_vectors.shape\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train_rf, X_test_rf, y_train_rf, y_test_rf = train_test_split(doc_vectors, rf_data.difficulty,\n",
        "                                                    test_size=0.2, random_state=0)\n",
        "\n"
      ],
      "metadata": {
        "id": "qhDT6-RJT0rP"
      },
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit model on training set\n",
        "rfc.fit(X_train_rf, y_train_rf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z2f31HiaVNq3",
        "outputId": "76ef82b4-dcfe-42d5-e5d2-b59772361bb7"
      },
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier()"
            ]
          },
          "metadata": {},
          "execution_count": 150
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_rf = rfc.predict(X_test_rf)\n",
        "\n",
        "evaluate(y_test_rf, y_pred_rf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4klckEeUVR1Q",
        "outputId": "f51f1757-e4ad-456f-efb0-e91e3b88e7f0"
      },
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ACCURACY SCORE:\n",
            "0.3677\n",
            "CLASSIFICATION REPORT:\n",
            "\tPrecision: 0.3664\n",
            "\tRecall: 0.3680\n",
            "\tF1_Score: 0.3653\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3> 5.3.3 Dimensionality Reduction"
      ],
      "metadata": {
        "id": "xEVmT3aGsvkS"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZFsQtcUMIpim"
      },
      "source": [
        "pca = PCA(n_components=200) #n_components can be varied to try out different models"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evMidJmPeMJ7"
      },
      "source": [
        "X_rf = rf_data['sentence']\n",
        "ylabels_rf = rf_data['difficulty']\n",
        "\n",
        "X_train_rf, X_test_rf, y_train_rf, y_test_rf = train_test_split(X_rf, ylabels_rf, test_size=0.2, random_state=0, stratify=ylabels_rf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_vec_rf = tfidf_vector.fit_transform(X_train_rf).toarray()\n",
        "X_test_vec_rf = tfidf_vector.transform(X_test_rf).toarray()\n",
        "print(X_train_vec_rf.shape)\n",
        "X_train_vec_rf"
      ],
      "metadata": {
        "id": "Cp6Tg7tzs1fC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#build pipe without Scaler & PCA\n",
        "scaler = StandardScaler()\n",
        "pipe = Pipeline([\n",
        "                 ('rfc', rfc),\n",
        "                 ])\n",
        "\n",
        "# Fit model\n",
        "start = time.time()\n",
        "pipe.fit(X_train_vec_rf, y_train_rf)\n",
        "end = time.time()\n",
        "print('Time: ', round(end-start, 4))\n",
        "print('Train Accuracy: ', round(pipe.score(X_train_vec_rf, y_train_rf), 4))\n",
        "print('Test Accuracy: ', round(pipe.score(X_test_vec_rf, y_test_rf), 4))"
      ],
      "metadata": {
        "id": "prCEfZefzFaf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#build pipe with  StandardScaler\n",
        "scaler = StandardScaler()\n",
        "pipe = Pipeline([\n",
        "                 ('scaler', scaler),\n",
        "                 ('rfc', rfc),\n",
        "                 ])\n",
        "\n",
        "# Fit model\n",
        "start = time.time()\n",
        "pipe.fit(X_train_vec_rf, y_train_rf)\n",
        "end = time.time()\n",
        "print('Time: ', round(end-start, 4))\n",
        "print('Train Accuracy: ', round(pipe.score(X_train_vec_rf, y_train_rf), 4))\n",
        "print('Test Accuracy: ', round(pipe.score(X_test_vec_rf, y_test_rf), 4))"
      ],
      "metadata": {
        "id": "BfARcuclvVQx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#build pipe with PCA \n",
        "pipe = Pipeline([\n",
        "                 ('pca', pca),\n",
        "                 ('rfc', rfc),\n",
        "                 ])\n",
        "\n",
        "# Fit model\n",
        "start = time.time()\n",
        "pipe.fit(X_train_vec_rf, y_train_rf)\n",
        "end = time.time()\n",
        "print('Time: ', round(end-start, 4))\n",
        "print('Train Accuracy: ', round(pipe.score(X_train_vec_rf, y_train_rf), 4))\n",
        "print('Test Accuracy: ', round(pipe.score(X_test_vec_rf, y_test_rf), 4))"
      ],
      "metadata": {
        "id": "RTI2CafMtD9x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#build pipe with PCA & StandardScaler\n",
        "\n",
        "pipe = Pipeline([\n",
        "                 ('scaler', scaler),\n",
        "                 ('pca', pca),\n",
        "                 ('rfc', rfc),\n",
        "                 ])\n",
        "\n",
        "# Fit model\n",
        "start = time.time()\n",
        "pipe.fit(X_train_vec_rf, y_train_rf)\n",
        "end = time.time()\n",
        "print('Time: ', round(end-start, 4))\n",
        "print('Train Accuracy: ', round(pipe.score(X_train_vec_rf, y_train_rf), 4))\n",
        "print('Test Accuracy: ', round(pipe.score(X_test_vec_rf, y_test_rf), 4))"
      ],
      "metadata": {
        "id": "WInkSWaAuI5X"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}