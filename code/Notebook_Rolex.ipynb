{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Notebook_Rolex.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chadi-aebi/DMML2021_Rolex/blob/main/code/Notebook_Rolex.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ee1dPZ5x3h76"
      },
      "source": [
        "<h1> UNIL Team Rolex\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook we will proceed as follows:\n",
        "\n",
        "First we prepare the notebook by importing essential methods and components for text analytics. Then we will start with some preparations such as building a tokenizer with different possible features to apply this later in the classification.\n",
        "\n",
        "Subsequently, we start the text analytics divided by the different classifiers starting with a baseline calculation.\n",
        "Each classifier section starts with a classification without any data preprocessing or other features. Then we tune the hyperparameters for the classifier to find the best parameters. After that, we train models that also implement the preprocessing of data, we try word embeddings and in the end we try out dimensionality reduction and standardisation.\n",
        "\n",
        "The notebook has the following chapters:\n",
        "\n",
        "\n",
        "\n",
        "*   0.1.   Preparation to start working - impor necessary methods etc.\n",
        "*   0.2.   Further preparation for classification\n",
        "\n",
        "\n",
        "1.     Baseline calculation\n",
        "2.     Logistic Regression\n",
        "3.     kNN Classifier\n",
        "4.     Decision Tree\n",
        "5.     Random Forests\n",
        "6.     Linear Support Vector Classification\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wL9btvP1beMI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 0.1 Preparation to start working - import necessary methods etc."
      ],
      "metadata": {
        "id": "3TkfqWbwbecy"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gNRi80vlEyt1"
      },
      "source": [
        "#Install and update spacy\n",
        "!pip install -U spacy\n",
        "#Download the french language model\n",
        "!python -m spacy download fr_core_news_sm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sp-iRswNE44F"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import spacy\n",
        "import string\n",
        "import csv\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "leHXJLnvC3c2"
      },
      "source": [
        "#Classifier\n",
        "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "#Other\n",
        "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder, LabelEncoder, StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.utils.multiclass import unique_labels\n",
        "from spacy import displacy\n",
        "from spacy.lang.fr.stop_words import STOP_WORDS\n",
        "from spacy.lang.fr import French"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XBpIhxQR7iuC"
      },
      "source": [
        "# 0.2 Further preparations to starkt with classification\n",
        "\n",
        "Set random_seed, Vectorizers without preprocessing and load the french language model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pCDvQDBjHb2P"
      },
      "source": [
        "np.random_seed = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FS9S8XIjQUa1"
      },
      "source": [
        "#Set TF-IDF and Count Vectorizer without any more specifications\n",
        "tfidf_vector = TfidfVectorizer()\n",
        "#count_vector = CountVectorizer()\n",
        "\n",
        "#with preprocessing\n",
        "#tfidf_vector = TfidfVectorizer(tokenizer=spacy_tokenizer)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzA0kkN1EKG2"
      },
      "source": [
        "#Load the french language model\n",
        "nlp = spacy.load('fr_core_news_sm')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jiLVW789Q1HU"
      },
      "source": [
        "#Import stop words from french language model and puncutations\n",
        "stop_words=spacy.lang.fr.stop_words.STOP_WORDS\n",
        "punctuations = string.punctuation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0efssDo2QJ-O"
      },
      "source": [
        "#Create a tokenizer function that can be used for preprocessing the data for classification - we try out different combinations of the sentence features\n",
        "\n",
        "def spacy_tokenizer(sentence):\n",
        "    # Create token object, which is used to create documents with linguistic annotations.\n",
        "    mytokens = nlp(sentence)\n",
        "\n",
        "    # Lemmatize each token and convert each token into lowercase\n",
        "    mytokens = [ word.lemma_.lower().strip() for word in mytokens ]\n",
        "\n",
        "    # Remove stop words \n",
        "    #mytokens = [ word for word in mytokens if word not in stop_words]\n",
        "\n",
        "    # Remove punctuation\n",
        "    #mytokens = [ word for word in mytokens if word not in punctuations ]\n",
        "\n",
        "    # Remove anonymous dates and people\n",
        "    mytokens = [ word.replace('xx/', '').replace('xxxx/', '').replace('xx', '') for word in mytokens ]\n",
        "    mytokens = [ word for word in mytokens if word not in [\"xxxx\", \"xx\", \"\"]]\n",
        "\n",
        "    # Return preprocessed list of tokens\n",
        "    return mytokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We found that stopword removal did not lead to better results. This is well possible because by removing frequent and rather simple words you might remove the majority of words that appear in sentences of A1/A2 difficulty. Without those words it will be difficult to differentiate between sentences that are more sophisticated and those that only stay at a very basic level. "
      ],
      "metadata": {
        "id": "4w06MqoqeCRL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Function for model evaluation\n",
        "def evaluate(true, pred):\n",
        "    precision = precision_score(true, pred, average= 'macro')\n",
        "    recall = recall_score(true, pred, average = 'macro')\n",
        "    f1 = f1_score(true, pred, average = 'macro')\n",
        "    print(f\"ACCURACY SCORE:\\n{accuracy_score(true, pred):.4f}\")\n",
        "    print(f\"CLASSIFICATION REPORT:\\n\\tPrecision: {precision:.4f}\\n\\tRecall: {recall:.4f}\\n\\tF1_Score: {f1:.4f}\")\n",
        "\n",
        "\n",
        " # Confusion Matrix\n",
        "\n",
        " # Evaluate model\n",
        "def accuracy_conf_mat(y_test, y_pred):\n",
        "  print(round(accuracy_score(y_test, y_pred), 4))\n",
        "  labels = list(unique_labels(y_test, y_pred))\n",
        "  conf_mat = confusion_matrix(y_test, y_pred, labels = labels)\n",
        "  fig, ax = plt.subplots(figsize=(10,10))\n",
        "  plt.title('Confusion matrix of the classifier')\n",
        "  sns.heatmap(conf_mat, annot=True, fmt='d', xticklabels = labels, yticklabels = labels)\n",
        "  plt.ylabel('Actual')\n",
        "  plt.xlabel('Predicted')\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "\n",
        " "
      ],
      "metadata": {
        "id": "jjRObxuTZwsB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>  Getting started - text analytics per classifier"
      ],
      "metadata": {
        "id": "5HYObhzWi8Xa"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DbxvE-3a4b55"
      },
      "source": [
        "\n",
        "# 1. Baseline\n",
        "\n",
        "First, we start by calculating the baseline."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4vHFy3fGozP"
      },
      "source": [
        "data=pd.read_csv('https://raw.githubusercontent.com/chadi-aebi/DMML2021_Rolex/main/data/training_data.csv', index_col='id')\n",
        "X = data['sentence']\n",
        "ylabels = data['difficulty']\n",
        "print(ylabels.value_counts(normalize=True))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "baseline = ylabels.value_counts(normalize = True).max()\n",
        "baseline"
      ],
      "metadata": {
        "id": "72ZblPn4OfEA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Logistic Regression\n",
        "<h2> 2.1 Logistic Regression without any data cleaning or tuning"
      ],
      "metadata": {
        "id": "7drQ-3Uzo2XT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We load the data..."
      ],
      "metadata": {
        "id": "pgAcP-acZ_cL"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ZA9m_yvB_vQ"
      },
      "source": [
        "lr_data=pd.read_csv('https://raw.githubusercontent.com/chadi-aebi/DMML2021_Rolex/main/data/training_data.csv', index_col='id')\n",
        "lr_test_df = pd.read_csv('https://raw.githubusercontent.com/chadi-aebi/DMML2021_Rolex/main/data/unlabelled_test_data.csv', index_col='id')\n",
        "lr_data.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr_data.head()"
      ],
      "metadata": {
        "id": "2PmZ981bYkxe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "..., select the features and train-test-split the data."
      ],
      "metadata": {
        "id": "LbLLdT9RaC75"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3tILch0iP2cj"
      },
      "source": [
        "X_lr = lr_data['sentence']\n",
        "ylabels_lr = lr_data['difficulty']\n",
        "\n",
        "X_train_lr, X_test_lr, y_train_lr, y_test_lr = train_test_split(X_lr, ylabels_lr, test_size=0.2, random_state=0, stratify=ylabels_lr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we define the classifier with its default settings..."
      ],
      "metadata": {
        "id": "fYfcwLCpam1c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define classifier\n",
        "lreg = LogisticRegression()"
      ],
      "metadata": {
        "id": "miLu_FfflSgQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "... and create the pipeline with the Classifier and the TF-IDF vectorizer without tokenization. Then we fit the model to our training data..."
      ],
      "metadata": {
        "id": "phD9QiDYan4-"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jf3Pp3f9P-Ue"
      },
      "source": [
        "# Create pipeline\n",
        "pipe = Pipeline([('vectorizer', tfidf_vector),\n",
        "                 ('classifier', lreg)])\n",
        "\n",
        "# Fit model on training set\n",
        "pipe.fit(X_train_lr, y_train_lr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "... and make predicitions with this test data."
      ],
      "metadata": {
        "id": "td6YESpUa3_Z"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ggeYhJXwRtFU"
      },
      "source": [
        "# Predictions\n",
        "y_pred_lr = pipe.predict(X_test_lr)\n",
        "\n",
        "acc_lr_0 = accuracy_score(y_test_lr,y_pred_lr)\n",
        "evaluate(y_test_lr, y_pred_lr)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_conf_mat(y_test_lr, y_pred_lr)\n"
      ],
      "metadata": {
        "id": "rlVXaCHqS8PQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This was a first model without any other features. Let's have a look at some wrong predictions to find some hints what could be improved."
      ],
      "metadata": {
        "id": "nEgUC_jQrMJ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(X_test_lr, columns=[\"sentence\"])\n",
        "df[\"actual\"] = y_test_lr\n",
        "df[\"predicted\"] = y_pred_lr\n",
        "\n",
        "incorrect = df[df[\"actual\"] != df[\"predicted\"]]\n",
        "incorrect.head(20)"
      ],
      "metadata": {
        "id": "NrpKDcxtrVLg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#submission_test_lr = pd.DataFrame(y_pred_lr, columns=['difficulty'])\n",
        "#submission_test_lr"
      ],
      "metadata": {
        "id": "nG_lAVrfgCLc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#submission_test_lr.to_csv('submission_21-12-12.csv')"
      ],
      "metadata": {
        "id": "aXqVq5PWgHqd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2> 2.2 Logistic Regression with hyperparameters tuning"
      ],
      "metadata": {
        "id": "mXbzubYRpAFR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section we searched for the best hyperparameters to improve our classifiers."
      ],
      "metadata": {
        "id": "Hheo85zWbAEE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define parameters to test\n",
        "\n",
        "#grid_lr = {\n",
        "#    'C': [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1, 10, 100],\n",
        "#    'penalty': ['none', 'l1', 'l2', 'elasticnet'],\n",
        "#    #'max_iter': list(range(100,800,100)),\n",
        "#    'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\n",
        "#}\n",
        "\n",
        "# Define and fit model\n",
        "\n",
        "#lreg = LogisticRegression()\n",
        "#lreg_cv = GridSearchCV(lreg, grid_lr, cv=10)\n",
        "#pipe = Pipeline([('vectorizer', tfidf_vector),\n",
        " #                ('classifier', lreg_cv)])\n",
        "\n",
        "#pipe.fit(X_train_lr, y_train_lr)\n",
        "\n",
        "# Print results\n",
        "\n",
        "#print(\"Hyperparameters:\", lreg_cv.best_params_)\n",
        "#y_pred_lr = pipe.predict(X_test_lr)\n",
        "#evaluate(y_test_lr, y_pred_lr)"
      ],
      "metadata": {
        "id": "weEZ4nUcAb2E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We saved the hyperparameters and use them for the following predictions."
      ],
      "metadata": {
        "id": "3vvShqFYbGvN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#safe best parameters\n",
        "#Hyperparameters= {'C': 10, 'penalty': 'l2', 'solver': 'liblinear'}"
      ],
      "metadata": {
        "id": "n5qjNnn_OJYT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2> 2.3 Logistic Regression with further methods "
      ],
      "metadata": {
        "id": "h5W6Q_50pLnr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3> 2.3.1 Preprocessing"
      ],
      "metadata": {
        "id": "F6Pcp8IkfdPg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section we extended the TF-IDF Vectorizer with the tokenizer we set at the beginning. By commenting some lines of the tokenizer we could try different settings and in the end keep the best combination."
      ],
      "metadata": {
        "id": "soqoTxPIcN3J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_vec_lr = TfidfVectorizer(tokenizer=spacy_tokenizer)"
      ],
      "metadata": {
        "id": "nN32JGDmNeOj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define classifier with best hyperparameters\n",
        "lreg = LogisticRegression(C=10, penalty = 'l2', solver = 'liblinear')"
      ],
      "metadata": {
        "id": "-1Daf_BsYVlM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create pipeline\n",
        "pipe = Pipeline([('vectorizer', tfidf_vec_lr),\n",
        "                 ('classifier', lreg)])\n",
        "\n",
        "# Fit model on training set\n",
        "pipe.fit(X_train_lr, y_train_lr)"
      ],
      "metadata": {
        "id": "dU4aRGsfNkMx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predictions\n",
        "y_pred_lr = pipe.predict(X_test_lr)\n",
        "\n",
        "#accuracy_score(y_test_lr,y_pred_lr)\n",
        "evaluate(y_test_lr, y_pred_lr)"
      ],
      "metadata": {
        "id": "Ccjxp0RgNn4t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To make the best predictions for the kaggle data set, we take the parameters and settings from our best model and apply fit the model on the whole dataset."
      ],
      "metadata": {
        "id": "NR0416aXky8q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create pipeline\n",
        "pipe = Pipeline([('vectorizer', tfidf_vec_lr),\n",
        "                 ('classifier', lreg)])\n",
        "\n",
        "# Fit model on whole data set\n",
        "pipe.fit(X_lr, ylabels_lr)"
      ],
      "metadata": {
        "id": "xmlZ2E40N9b1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_test=pipe.predict(lr_test_df['sentence'])"
      ],
      "metadata": {
        "id": "w3HeE12oOo1z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to submit the predictions we save the predictions and create a csv file."
      ],
      "metadata": {
        "id": "JWP5FzcXlFaq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "submission_test = pd.DataFrame(y_pred_test, columns=['difficulty'])\n",
        "#submission_test"
      ],
      "metadata": {
        "id": "DPzps3KgOUnC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission_test.to_csv('submission_21-12-16_5.csv')"
      ],
      "metadata": {
        "id": "_kbWTxmAOXrr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3> 2.3.2 Word Embeddings"
      ],
      "metadata": {
        "id": "7RlGv4wtfbFB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this part, we use Word Embeddings instead of Tokenization. The code is modeled after the Kaggle course we saw in class. "
      ],
      "metadata": {
        "id": "UsODN9aockRv"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd858b2e-d700-41f3-c049-f825078ace0d",
        "id": "qpc29nO2YLbt"
      },
      "source": [
        "#Vectorizing - Word Embeddings\n",
        "\n",
        "with nlp.disable_pipes():\n",
        "    doc_vectors = np.array([nlp(sentence).vector for sentence in lr_data.sentence])\n",
        "    \n",
        "doc_vectors.shape\n",
        "\n"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(4800, 96)"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train_lr, X_test_lr, y_train_lr, y_test_lr = train_test_split(doc_vectors, lr_data.difficulty,\n",
        "                                                    test_size=0.2, random_state=0)\n",
        "\n"
      ],
      "metadata": {
        "id": "74FgTRZfYLbv"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit model on training set\n",
        "lreg.fit(X_train_lr, y_train_lr)"
      ],
      "metadata": {
        "outputId": "bde20a9f-974c-495b-ccfd-d7b8d7791f94",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s3q9HgldYLbw"
      },
      "execution_count": 34,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "LogisticRegression(C=10, solver='liblinear')"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_lr = lreg.predict(X_test_lr)\n",
        "\n",
        "evaluate(y_test_lr, y_pred_lr)"
      ],
      "metadata": {
        "outputId": "d5c84497-79dd-47a4-8d48-6baeada2482b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FLhgqt1qYLby"
      },
      "execution_count": 35,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ACCURACY SCORE:\n",
            "0.3927\n",
            "CLASSIFICATION REPORT:\n",
            "\tPrecision: 0.3890\n",
            "\tRecall: 0.3938\n",
            "\tF1_Score: 0.3889\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3> 2.3.3 Dimensionality Reduction"
      ],
      "metadata": {
        "id": "hFKj5PD_fVXM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, we apply standardization and dimensionality reduction and the combination of both."
      ],
      "metadata": {
        "id": "ANWgSzNnjFif"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pca = PCA(n_components=2800) #n_components can be varied to try out different models"
      ],
      "metadata": {
        "id": "q8dWBab9fZO3"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QhHZ3Oe4d6Ju"
      },
      "source": [
        "X_lr = lr_data['sentence']\n",
        "ylabels_lr = lr_data['difficulty']\n",
        "\n",
        "X_train_lr, X_test_lr, y_train_lr, y_test_lr = train_test_split(X_lr, ylabels_lr, test_size=0.2, random_state=0, stratify=ylabels_lr)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fit & transform data without preprocessing ..."
      ],
      "metadata": {
        "id": "g_dFIjgPkF7X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#X_train_vec_lr = tfidf_vector.fit_transform(X_train_lr).toarray()\n",
        "#X_test_vec_lr = tfidf_vector.transform(X_test_lr).toarray()\n",
        "#print(X_train_vec_lr.shape)"
      ],
      "metadata": {
        "id": "ATuNK3q1fwGX"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "...or fit & transform data with preprocessing (choose one for further classification)"
      ],
      "metadata": {
        "id": "OEPtVdMPkDdA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_vec_lr = tfidf_vec_lr.fit_transform(X_train_lr).toarray()\n",
        "X_test_vec_lr = tfidf_vec_lr.transform(X_test_lr).toarray()\n",
        "print(X_train_vec_lr.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0852899-d2d9-4e09-c631-e06b75e46673",
        "id": "MIDzYfWKizwz"
      },
      "execution_count": 39,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(3840, 9602)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#build pipe without Scaler & PCA\n",
        "scaler = StandardScaler()\n",
        "pipe = Pipeline([\n",
        "                 ('lreg', lreg),\n",
        "                 ])\n",
        "\n",
        "# Fit model\n",
        "pipe.fit(X_train_vec_lr, y_train_lr)\n",
        "print('Train Accuracy: ', round(pipe.score(X_train_vec_lr, y_train_lr), 4))\n",
        "print('Test Accuracy: ', round(pipe.score(X_test_vec_lr, y_test_lr), 4))"
      ],
      "metadata": {
        "id": "PeNEE4iIf6s-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a97a79a-c7b1-4db3-a6ec-a78d37c3d5ef"
      },
      "execution_count": 40,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Accuracy:  0.9732\n",
            "Test Accuracy:  0.4938\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#build pipe with  StandardScaler\n",
        "scaler = StandardScaler()\n",
        "pipe = Pipeline([\n",
        "                 ('scaler', scaler),\n",
        "                 ('lreg', lreg),\n",
        "                 ])\n",
        "\n",
        "# Fit model\n",
        "pipe.fit(X_train_vec_lr, y_train_lr)\n",
        "print('Train Accuracy: ', round(pipe.score(X_train_vec_lr, y_train_lr), 4))\n",
        "print('Test Accuracy: ', round(pipe.score(X_test_vec_lr, y_test_lr), 4))"
      ],
      "metadata": {
        "id": "Z6KEJ87CgMnE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea6cea16-f7b7-4d4d-af39-e247eeded4c1"
      },
      "execution_count": 41,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Accuracy:  0.999\n",
            "Test Accuracy:  0.3969\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#build pipe with PCA \n",
        "pipe = Pipeline([\n",
        "                 ('pca', pca),\n",
        "                 ('lreg', lreg),\n",
        "                 ])\n",
        "\n",
        "# Fit model\n",
        "\n",
        "pipe.fit(X_train_vec_lr, y_train_lr)\n",
        "print('Train Accuracy: ', round(pipe.score(X_train_vec_lr, y_train_lr), 4))\n",
        "print('Test Accuracy: ', round(pipe.score(X_test_vec_lr, y_test_lr), 4))"
      ],
      "metadata": {
        "id": "RmSXgfaogbvC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b13f027-d504-430a-9fd0-026c84276412"
      },
      "execution_count": 43,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Accuracy:  0.9615\n",
            "Test Accuracy:  0.4979\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#build pipe with PCA & StandardScaler\n",
        "\n",
        "pipe = Pipeline([\n",
        "                 ('scaler', scaler),\n",
        "                 ('pca', pca),\n",
        "                 ('lreg', lreg),\n",
        "                 ])\n",
        "\n",
        "# Fit model\n",
        "\n",
        "pipe.fit(X_train_vec_lr, y_train_lr)\n",
        "print('Train Accuracy: ', round(pipe.score(X_train_vec_lr, y_train_lr), 4))\n",
        "print('Test Accuracy: ', round(pipe.score(X_test_vec_lr, y_test_lr), 4))"
      ],
      "metadata": {
        "id": "HE_xoFmFgn7J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "adcbaa70-ef90-4f51-9e85-aea1b0d6c51c"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy:  0.999\n",
            "Test Accuracy:  0.424\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following parts are structured the same way but the classifier changes. The given explanation for the code is therefore more or less still valid for the following sections."
      ],
      "metadata": {
        "id": "t_D1CBQajNQY"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJp_X2jV6lP4"
      },
      "source": [
        "# 3. kNN\n",
        "<h2> 3.1 kNN without any data cleaning or tuning\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KmCSWQGs68Eq"
      },
      "source": [
        "knn_data=pd.read_csv('https://raw.githubusercontent.com/chadi-aebi/DMML2021_Rolex/main/data/training_data.csv', index_col='id')\n",
        "knn_test_df = pd.read_csv('https://raw.githubusercontent.com/chadi-aebi/DMML2021_Rolex/main/data/unlabelled_test_data.csv', index_col='id')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPjANU9g7LiI"
      },
      "source": [
        "X_knn = knn_data['sentence']\n",
        "ylabels_knn = knn_data['difficulty']\n",
        "\n",
        "X_train_knn, X_test_knn, y_train_knn, y_test_knn = train_test_split(X_knn, ylabels_knn, test_size=0.2, random_state=0, stratify=ylabels_knn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define classifier\n",
        "knn = KNeighborsClassifier()\n"
      ],
      "metadata": {
        "id": "X65tlM6cl1UP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ys-DfQ9Qg358"
      },
      "source": [
        "# Create pipeline\n",
        "pipe = Pipeline([('vectorizer', tfidf_vector),\n",
        "                 ('classifier', knn)])\n",
        "\n",
        "# Fit model on training set\n",
        "pipe.fit(X_train_knn, y_train_knn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPglJHd8gsVc"
      },
      "source": [
        "y_pred_knn = pipe.predict(X_test_knn)\n",
        "\n",
        "acc_knn_0 = accuracy_score(y_test_knn,y_pred_knn)\n",
        "evaluate(y_test_knn, y_pred_knn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_conf_mat(y_test_knn, y_pred_knn)"
      ],
      "metadata": {
        "id": "n7QVM1URhoyP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2> 3.2 kNN with hyperparameter tuning\n"
      ],
      "metadata": {
        "id": "E9TvPasnpxAF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define parameters to test\n",
        "\n",
        "#grid_knn = {'n_neighbors':np.arange(1,100),\n",
        " #       'p':np.arange(1,3),\n",
        " #       'weights':['uniform','distance']\n",
        " #      }\n",
        "\n",
        "# Define and fit model\n",
        "\n",
        "#knn = KNeighborsClassifier()\n",
        "#knn_cv = GridSearchCV(knn, grid, cv=10)\n",
        "#pipe = Pipeline([('vectorizer', tfidf_vector),\n",
        " #                ('classifier', knn_cv)])\n",
        "\n",
        "#pipe.fit(X_train_knn, y_train_knn)\n",
        "\n",
        "# Print results\n",
        "\n",
        "#print(\"Hyperparameters:\", knn_cv.best_params_)\n",
        "#y_pred_knn = pipe.predict(X_test_knn)\n",
        "#evaluate(y_test_knn, y_pred_knn)"
      ],
      "metadata": {
        "id": "4XN_daqtn_36"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#bestparams_knn = {'n_neighbors': 82, 'p': 2, 'weights': 'distance'}"
      ],
      "metadata": {
        "id": "VqiIDe_tuXZV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2> 3.3 kNN with further methods"
      ],
      "metadata": {
        "id": "iMRoTJgxp2vZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3> 3.3.1 Preprocessing"
      ],
      "metadata": {
        "id": "Lwa2zJYUli44"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Define classifier with best parameters\n",
        "knn = KNeighborsClassifier(n_neighbors= 82, weights= 'distance')"
      ],
      "metadata": {
        "id": "dncB-Z4vXl7c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_vec_knn = TfidfVectorizer(tokenizer=spacy_tokenizer)"
      ],
      "metadata": {
        "id": "nR0awg_kKBDB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create pipeline\n",
        "pipe = Pipeline([('vectorizer', tfidf_vec_knn),\n",
        "                 ('classifier', knn)])\n",
        "\n",
        "# Fit model on training set\n",
        "pipe.fit(X_train_knn, y_train_knn)"
      ],
      "metadata": {
        "id": "dHTpmohvKBDD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predictions\n",
        "y_pred_knn = pipe.predict(X_test_knn)\n",
        "\n",
        "evaluate(y_test_knn, y_pred_knn)"
      ],
      "metadata": {
        "id": "CPahV5I_KBDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3> 3.3.2 Word Embeddings"
      ],
      "metadata": {
        "id": "e07_-cDOlrUq"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "soVJLmmEXX81"
      },
      "source": [
        "#Vectorizing - Word Embeddings\n",
        "\n",
        "with nlp.disable_pipes():\n",
        "    doc_vectors = np.array([nlp(sentence).vector for sentence in knn_data.sentence])\n",
        "    \n",
        "doc_vectors.shape\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train_knn, X_test_knn, y_train_knn, y_test_knn = train_test_split(doc_vectors, knn_data.difficulty,\n",
        "                                                    test_size=0.2, random_state=0)\n",
        "\n"
      ],
      "metadata": {
        "id": "8tU4aFhxXX85"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit model on training set\n",
        "knn.fit(X_train_knn, y_train_knn)"
      ],
      "metadata": {
        "id": "MlQtb4MhXX87"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_knn = knn.predict(X_test_knn)\n",
        "\n",
        "evaluate(y_test_knn, y_pred_knn)"
      ],
      "metadata": {
        "id": "pKJNKWXGXX89"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3> 3.3.3 Dimensionality Reduction"
      ],
      "metadata": {
        "id": "JC2JGEHNlum0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pca = PCA(n_components=200) #n_components can be varied to try out different models"
      ],
      "metadata": {
        "id": "yKiXFBeAmF7F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DiAb3k01d_JM"
      },
      "source": [
        "X_knn = knn_data['sentence']\n",
        "ylabels_knn = knn_data['difficulty']\n",
        "\n",
        "X_train_knn, X_test_knn, y_train_knn, y_test_knn = train_test_split(X_knn, ylabels_knn, test_size=0.2, random_state=0, stratify=ylabels_knn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_vec_knn = tfidf_vector.fit_transform(X_train_knn).toarray()\n",
        "X_test_vec_knn = tfidf_vector.transform(X_test_knn).toarray()\n",
        "print(X_train_vec_knn.shape)"
      ],
      "metadata": {
        "id": "LbN0__1kmF7G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#build pipe without Scaler & PCA\n",
        "scaler = StandardScaler()\n",
        "pipe = Pipeline([\n",
        "                 ('knn', knn),\n",
        "                 ])\n",
        "\n",
        "# Fit model\n",
        "pipe.fit(X_train_vec_knn, y_train_knn)\n",
        "print('Train Accuracy: ', round(pipe.score(X_train_vec_knn, y_train_knn), 4))\n",
        "print('Test Accuracy: ', round(pipe.score(X_test_vec_knn, y_test_knn), 4))"
      ],
      "metadata": {
        "id": "g4Z_UnJTlyQm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#build pipe with  StandardScaler\n",
        "scaler = StandardScaler()\n",
        "pipe = Pipeline([\n",
        "                 ('scaler', scaler),\n",
        "                 ('knn', knn),\n",
        "                 ])\n",
        "\n",
        "# Fit model\n",
        "pipe.fit(X_train_vec_knn, y_train_knn)\n",
        "print('Train Accuracy: ', round(pipe.score(X_train_vec_knn, y_train_knn), 4))\n",
        "print('Test Accuracy: ', round(pipe.score(X_test_vec_knn, y_test_knn), 4))"
      ],
      "metadata": {
        "id": "1bwqJb6jlyQo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#build pipe with PCA \n",
        "pipe = Pipeline([\n",
        "                 ('pca', pca),\n",
        "                 ('knn', knn),\n",
        "                 ])\n",
        "\n",
        "\n",
        "# Fit model\n",
        "pipe.fit(X_train_vec_knn, y_train_knn)\n",
        "print('Train Accuracy: ', round(pipe.score(X_train_vec_knn, y_train_knn), 4))\n",
        "print('Test Accuracy: ', round(pipe.score(X_test_vec_knn, y_test_knn), 4))"
      ],
      "metadata": {
        "id": "L1CW1RzxlyQq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#build pipe with PCA & StandardScaler\n",
        "\n",
        "pipe = Pipeline([\n",
        "                 ('scaler', scaler),\n",
        "                 ('pca', pca),\n",
        "                 ('knn', knn),\n",
        "                 ])\n",
        "\n",
        "# Fit model\n",
        "pipe.fit(X_train_vec_knn, y_train_knn)\n",
        "print('Train Accuracy: ', round(pipe.score(X_train_vec_knn, y_train_knn), 4))\n",
        "print('Test Accuracy: ', round(pipe.score(X_test_vec_knn, y_test_knn), 4))"
      ],
      "metadata": {
        "id": "bKQ4lwWqlyQs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYol3GFa6wzs"
      },
      "source": [
        "# 4. Decision Tree\n",
        "<h2> 4.1 Decision Tree without any data cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7VcEoDF-6_rZ"
      },
      "source": [
        "tree_data=pd.read_csv('https://raw.githubusercontent.com/chadi-aebi/DMML2021_Rolex/main/data/training_data.csv', index_col='id')\n",
        "tree_test_df = pd.read_csv('https://raw.githubusercontent.com/chadi-aebi/DMML2021_Rolex/main/data/unlabelled_test_data.csv', index_col='id')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oSPymGOk7PfG"
      },
      "source": [
        "X_tree = tree_data['sentence']\n",
        "ylabels_tree = tree_data['difficulty']\n",
        "\n",
        "X_train_tree, X_test_tree, y_train_tree, y_test_tree = train_test_split(X_tree, ylabels_tree, test_size=0.2, random_state=0, stratify=ylabels_tree)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define classifier\n",
        "tree = DecisionTreeClassifier()"
      ],
      "metadata": {
        "id": "zWfElWrvm15t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8sFIl3VHjWZk"
      },
      "source": [
        "\n",
        "# Create pipeline\n",
        "pipe = Pipeline([('vectorizer', tfidf_vector),\n",
        "                 ('classifier', tree)])\n",
        "\n",
        "# Fit model on training set\n",
        "pipe.fit(X_train_tree, y_train_tree)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGPV-iKDifr_"
      },
      "source": [
        "y_pred_tree = pipe.predict(X_test_tree)\n",
        "\n",
        "acc_tree_0 = accuracy_score(y_test_tree,y_pred_tree)\n",
        "evaluate(y_test_tree, y_pred_tree)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_conf_mat(y_test_tree, y_pred_tree)"
      ],
      "metadata": {
        "id": "5e8OdI3lht-A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2> 4.2 Decision Tree with hyperparameter tuning"
      ],
      "metadata": {
        "id": "B53JCf7GqIHD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Grid Search - hyperparameter tuning\n",
        "\n",
        "\n",
        "# Define parameters to test\n",
        "#grid_tree ={\"criterion\" : [\"gini\", \"entropy\"],\n",
        "#            \"splitter\":[\"best\",\"random\"],\n",
        "#            \"max_depth\" : [1,5,20,50,100, None],\n",
        "#           \"min_samples_leaf\":[1,5,20,50,100, None],\n",
        "#           \"max_features\":[\"auto\",\"log2\",\"sqrt\",None],\n",
        "#           \"max_leaf_nodes\":[None,10,50,100] }\n",
        "\n",
        "# Define and fit model\n",
        "#tree = tree = DecisionTreeClassifier()\n",
        "#tree_cv = GridSearchCV(tree, grid_tree, cv=10)\n",
        "#pipe = Pipeline([('vectorizer', tfidf_vector),\n",
        "#                 ('classifier', tree_cv)])\n",
        "\n",
        "#pipe.fit(X_train_tree, y_train_tree)\n",
        "\n",
        "# Print results\n",
        "#print(\"Hyperparameters:\", tree_cv.best_params_)\n",
        "#y_pred_tree = pipe.predict(X_test_tree)\n",
        "#evaluate(y_test_tree, y_pred_tree)"
      ],
      "metadata": {
        "id": "vWKVW9MsqCOB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We save the best parameters from the GridSearch to use it for further models:"
      ],
      "metadata": {
        "id": "_f2kKxaNx83u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#bestparams_tree = {'criterion': 'gini', 'max_depth': 20, 'max_features': None, 'max_leaf_nodes': 100, 'min_samples_leaf': 1, 'splitter': 'best'}"
      ],
      "metadata": {
        "id": "37yCmPd-wVhf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2> 4.3 Decision Tree with further methods"
      ],
      "metadata": {
        "id": "Z4cvQtWaqMgs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3> 4.3.1 Preprocessing\n"
      ],
      "metadata": {
        "id": "w5BmhwQTme4g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_vec_tree = TfidfVectorizer(tokenizer=spacy_tokenizer)"
      ],
      "metadata": {
        "id": "szhY96mrxGea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define classifier with best params\n",
        "tree = DecisionTreeClassifier(criterion = 'gini', max_depth = 20, max_features = None, max_leaf_nodes= 100, min_samples_leaf = 1, splitter = 'best')\n"
      ],
      "metadata": {
        "id": "E32C0uiowi8X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create pipeline\n",
        "pipe = Pipeline([('vectorizer', tfidf_vec_tree),\n",
        "                 ('classifier', tree)])\n",
        "\n",
        "# Fit model on training set\n",
        "pipe.fit(X_train_tree, y_train_tree)"
      ],
      "metadata": {
        "id": "lOh9FXOHXGv7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predictions\n",
        "y_pred_tree = pipe.predict(X_test_tree)\n",
        "\n",
        "evaluate(y_test_tree, y_pred_tree)"
      ],
      "metadata": {
        "id": "WqJwKV1Hxv8t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3> 4.3.2 Word embedding\n"
      ],
      "metadata": {
        "id": "eT2DTFqDmjH1"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "csRdtKxtW74V"
      },
      "source": [
        "#Vectorizing - Word Embeddings\n",
        "\n",
        "with nlp.disable_pipes():\n",
        "    doc_vectors = np.array([nlp(sentence).vector for sentence in tree_data.sentence])\n",
        "    \n",
        "doc_vectors.shape\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train_tree, X_test_tree, y_train_tree, y_test_tree = train_test_split(doc_vectors, tree_data.difficulty,\n",
        "                                                    test_size=0.2, random_state=0)\n",
        "\n"
      ],
      "metadata": {
        "id": "m6YwUAs6W74X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit model on training set\n",
        "tree.fit(X_train_tree, y_train_tree)"
      ],
      "metadata": {
        "id": "DJm5OuCpW74Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_tree = tree.predict(X_test_tree)\n",
        "\n",
        "evaluate(y_test_tree, y_pred_tree)"
      ],
      "metadata": {
        "id": "Odfi-C-EW74a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3> 4.3.3 Dimensionality Reduction"
      ],
      "metadata": {
        "id": "fqXWyJ4Dmp7q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pca = PCA(n_components=200) #n_components can be varied to try out different models"
      ],
      "metadata": {
        "id": "SI6deZ4ImzJD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8lvLlLvYeDUF"
      },
      "source": [
        "X_tree = tree_data['sentence']\n",
        "ylabels_tree = tree_data['difficulty']\n",
        "\n",
        "X_train_tree, X_test_tree, y_train_tree, y_test_tree = train_test_split(X_tree, ylabels_tree, test_size=0.2, random_state=0, stratify=ylabels_tree)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_vec_tree = tfidf_vector.fit_transform(X_train_tree).toarray()\n",
        "X_test_vec_tree = tfidf_vector.transform(X_test_tree).toarray()\n",
        "print(X_train_vec_tree.shape)"
      ],
      "metadata": {
        "id": "xzXxgI0gmzJG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#build pipe without Scaler & PCA\n",
        "scaler = StandardScaler()\n",
        "pipe = Pipeline([\n",
        "                 ('tree', tree),\n",
        "                 ])\n",
        "\n",
        "# Fit model\n",
        "pipe.fit(X_train_vec_tree, y_train_tree)\n",
        "print('Train Accuracy: ', round(pipe.score(X_train_vec_tree, y_train_tree), 4))\n",
        "print('Test Accuracy: ', round(pipe.score(X_test_vec_tree, y_test_tree), 4))"
      ],
      "metadata": {
        "id": "FQWNeBZtmzJI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#build pipe with  StandardScaler\n",
        "scaler = StandardScaler()\n",
        "pipe = Pipeline([\n",
        "                 ('scaler', scaler),\n",
        "                 ('tree', tree),\n",
        "                 ])\n",
        "# Fit model\n",
        "pipe.fit(X_train_vec_tree, y_train_tree)\n",
        "print('Train Accuracy: ', round(pipe.score(X_train_vec_tree, y_train_tree), 4))\n",
        "print('Test Accuracy: ', round(pipe.score(X_test_vec_tree, y_test_tree), 4))"
      ],
      "metadata": {
        "id": "63lmb9XGmzJK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#build pipe with PCA \n",
        "pipe = Pipeline([\n",
        "                 ('pca', pca),\n",
        "                 ('tree', tree),\n",
        "                 ])\n",
        "\n",
        "\n",
        "# Fit model\n",
        "pipe.fit(X_train_vec_tree, y_train_tree)\n",
        "print('Train Accuracy: ', round(pipe.score(X_train_vec_tree, y_train_tree), 4))\n",
        "print('Test Accuracy: ', round(pipe.score(X_test_vec_tree, y_test_tree), 4))"
      ],
      "metadata": {
        "id": "xlx1jX9kmzJK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#build pipe with PCA & StandardScaler\n",
        "\n",
        "pipe = Pipeline([\n",
        "                 ('scaler', scaler),\n",
        "                 ('pca', pca),\n",
        "                 ('tree', tree),\n",
        "                 ])\n",
        "\n",
        "# Fit model\n",
        "pipe.fit(X_train_vec_tree, y_train_tree)\n",
        "print('Train Accuracy: ', round(pipe.score(X_train_vec_tree, y_train_tree), 4))\n",
        "print('Test Accuracy: ', round(pipe.score(X_test_vec_tree, y_test_tree), 4))"
      ],
      "metadata": {
        "id": "XX8dynk5mzJL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zg-NDltT62B0"
      },
      "source": [
        "# 5. Random Forest\n",
        "<h2> 5.1 Random Forest without any data cleaning\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_IMuTls7C1j"
      },
      "source": [
        "rf_data=pd.read_csv('https://raw.githubusercontent.com/chadi-aebi/DMML2021_Rolex/main/data/training_data.csv', index_col='id')\n",
        "rf_test_df = pd.read_csv('https://raw.githubusercontent.com/chadi-aebi/DMML2021_Rolex/main/data/unlabelled_test_data.csv', index_col='id')\n",
        "#rf_test_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QkFia3ER7Ygu"
      },
      "source": [
        "X_rf = rf_data['sentence']\n",
        "ylabels_rf = rf_data['difficulty']\n",
        "\n",
        "X_train_rf, X_test_rf, y_train_rf, y_test_rf = train_test_split(X_rf, ylabels_rf, test_size=0.2, random_state=0, stratify=ylabels_rf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_rf_df = pd.DataFrame(X_train_rf)"
      ],
      "metadata": {
        "id": "dka8IrRm6dar"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0MTWwJkhh5p"
      },
      "source": [
        "# Define classifier\n",
        "rfc = RandomForestClassifier()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6c3bxy72SC3d"
      },
      "source": [
        "# Create pipeline with tfidf\n",
        "pipe = Pipeline([('vectorizer', tfidf_vector),\n",
        "                 ('classifier', rfc)])\n",
        "\n",
        "# Fit model on training set\n",
        "pipe.fit(X_train_rf, y_train_rf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_rf = pipe.predict(X_test_rf)\n",
        "\n",
        "acc_rf_0 = accuracy_score(y_test_rf,y_pred_rf)\n",
        "evaluate(y_test_rf, y_pred_rf)"
      ],
      "metadata": {
        "id": "6J2i2Fppq4L-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_conf_mat(y_test_rf, y_pred_rf)"
      ],
      "metadata": {
        "id": "DoB6r2kWhz5-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2> 5.2 Random Forest with Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "S8l4EDfrqcez"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In contrast to the other classifiers, we also tried RandomizedSearchCV for the Hyperparameter Tuning in this case. "
      ],
      "metadata": {
        "id": "ixe8BPL2b9hN"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uG3X64vahNoI"
      },
      "source": [
        "#Tuning Hyperparameters with RandomizedSearchCV\n",
        "\n",
        "# Number of trees in random forest\n",
        "#n_estimators = [int(x) for x in np.linspace(start = 500, stop = 2000, num = 10)]\n",
        "\n",
        "# Number of features to consider at every split\n",
        "#max_features = ['auto', 'sqrt']\n",
        "\n",
        "# Maximum number of levels in tree\n",
        "#max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
        "#max_depth.append(None)\n",
        "\n",
        "# Minimum number of samples required to split a node\n",
        "#min_samples_split = [2, 5, 10]\n",
        "\n",
        "# Minimum number of samples required at each leaf node\n",
        "#min_samples_leaf = [1, 2, 4]\n",
        "\n",
        "# Method of selecting samples for training each tree\n",
        "#bootstrap = [True, False]\n",
        "\n",
        "# Create the grid\n",
        "#grid_rf = {'n_estimators': n_estimators,\n",
        "#               'max_features': max_features,\n",
        "#               'max_depth': max_depth,\n",
        "#               'min_samples_split': min_samples_split,\n",
        "#               'min_samples_leaf': min_samples_leaf,\n",
        "#               'bootstrap': bootstrap}\n",
        "\n",
        "#print(random_grid)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UmbrlJcrhU8Y"
      },
      "source": [
        "#Crossvalidation with RandomizedSearchCV\n",
        "#rf_random = RandomizedSearchCV(estimator = rfc, param_distributions = random_grid_rf, n_iter = 10, cv = 5, verbose=2, random_state=0, n_jobs = -1)\n",
        "\n",
        "# Create pipeline with tfidf\n",
        "#pipe = Pipeline([('vectorizer', tfidf_vector),\n",
        " #                ('classifier', rf_random)])\n",
        "\n",
        "# Fit model on training set\n",
        "#pipe.fit(X_train_rf, y_train_rf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kipbly2yiLDe"
      },
      "source": [
        "#Save and display best parameters\n",
        "#best_param_1 = (bootstrap=False,\n",
        "# max_depth= 70,\n",
        "# max_features= 'auto',\n",
        "# min_samples_leaf= 1,\n",
        "# min_samples_split= 10,\n",
        "# n_estimators= 1166)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2> 5.3 Random Forest with further methods"
      ],
      "metadata": {
        "id": "D_e5qIOAqq9b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3> 5.3.1 Preprocessing"
      ],
      "metadata": {
        "id": "kmVABUvzrOyX"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gwkMbWKPlyiY"
      },
      "source": [
        "tfidf_vec_rf = TfidfVectorizer(tokenizer=spacy_tokenizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define classifier with best params - 07.12.2021 #2\n",
        "rfc = RandomForestClassifier(bootstrap=False,\n",
        " max_depth= 70,\n",
        " max_features= 'auto',\n",
        " min_samples_leaf= 1,\n",
        " min_samples_split= 10,\n",
        " n_estimators= 1166)"
      ],
      "metadata": {
        "id": "pP08OpFIOIoX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create pipeline with tfidf (Use whole dataset)\n",
        "pipe = Pipeline([('vectorizer', tfidf_vec_rf),\n",
        "                 ('classifier', rfc)])\n",
        "\n",
        "# Fit model on whole set\n",
        "pipe.fit(X_rf, ylabels_rf)"
      ],
      "metadata": {
        "id": "WSlZFnhxIOqJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create pipeline with tfidf\n",
        "pipe = Pipeline([('vectorizer', tfidf_vec_rf),\n",
        "                 ('classifier', rfc)])\n",
        "\n",
        "# Fit model on training set\n",
        "pipe.fit(X_train_rf, y_train_rf)"
      ],
      "metadata": {
        "id": "zQPTFI9lzjwr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I1tDTARLjbnd"
      },
      "source": [
        "y_pred_rf = pipe.predict(X_test_rf)\n",
        "\n",
        "#accuracy_score(y_test_rf,y_pred_rf)\n",
        "evaluate(y_test_rf, y_pred_rf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFIi0cuR_0Bh"
      },
      "source": [
        "#y_pred_test=pipe.predict(rf_test_df['sentence'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wv3TWRkWC02p"
      },
      "source": [
        "#submission_test = pd.DataFrame(y_pred_test, columns=['difficulty'])\n",
        "#submission_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aFMazOxqCbAO"
      },
      "source": [
        "\n",
        "#submission_test.to_csv('submission_21-12-13_2.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3> 5.3.2 Word Embeddings"
      ],
      "metadata": {
        "id": "6VQWY5NrrHFI"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bIrshLbcI1mk"
      },
      "source": [
        "#Vectorizing - Word Embeddings\n",
        "\n",
        "with nlp.disable_pipes():\n",
        "    doc_vectors = np.array([nlp(sentence).vector for sentence in rf_data.sentence])\n",
        "    \n",
        "doc_vectors.shape\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train_rf, X_test_rf, y_train_rf, y_test_rf = train_test_split(doc_vectors, rf_data.difficulty,\n",
        "                                                    test_size=0.2, random_state=0)\n",
        "\n"
      ],
      "metadata": {
        "id": "qhDT6-RJT0rP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit model on training set\n",
        "rfc.fit(X_train_rf, y_train_rf)"
      ],
      "metadata": {
        "id": "Z2f31HiaVNq3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_rf = rfc.predict(X_test_rf)\n",
        "\n",
        "evaluate(y_test_rf, y_pred_rf)"
      ],
      "metadata": {
        "id": "4klckEeUVR1Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3> 5.3.3 Dimensionality Reduction"
      ],
      "metadata": {
        "id": "xEVmT3aGsvkS"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZFsQtcUMIpim"
      },
      "source": [
        "pca = PCA(n_components=200) #n_components can be varied to try out different models"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evMidJmPeMJ7"
      },
      "source": [
        "X_rf = rf_data['sentence']\n",
        "ylabels_rf = rf_data['difficulty']\n",
        "\n",
        "X_train_rf, X_test_rf, y_train_rf, y_test_rf = train_test_split(X_rf, ylabels_rf, test_size=0.2, random_state=0, stratify=ylabels_rf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_vec_rf = tfidf_vector.fit_transform(X_train_rf).toarray()\n",
        "X_test_vec_rf = tfidf_vector.transform(X_test_rf).toarray()\n",
        "print(X_train_vec_rf.shape)"
      ],
      "metadata": {
        "id": "Cp6Tg7tzs1fC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#build pipe without Scaler & PCA\n",
        "scaler = StandardScaler()\n",
        "pipe = Pipeline([\n",
        "                 ('rfc', rfc),\n",
        "                 ])\n",
        "\n",
        "# Fit model\n",
        "start = time.time()\n",
        "pipe.fit(X_train_vec_rf, y_train_rf)\n",
        "end = time.time()\n",
        "print('Time: ', round(end-start, 4))\n",
        "print('Train Accuracy: ', round(pipe.score(X_train_vec_rf, y_train_rf), 4))\n",
        "print('Test Accuracy: ', round(pipe.score(X_test_vec_rf, y_test_rf), 4))"
      ],
      "metadata": {
        "id": "prCEfZefzFaf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#build pipe with  StandardScaler\n",
        "scaler = StandardScaler()\n",
        "pipe = Pipeline([\n",
        "                 ('scaler', scaler),\n",
        "                 ('rfc', rfc),\n",
        "                 ])\n",
        "\n",
        "# Fit model\n",
        "start = time.time()\n",
        "pipe.fit(X_train_vec_rf, y_train_rf)\n",
        "end = time.time()\n",
        "print('Time: ', round(end-start, 4))\n",
        "print('Train Accuracy: ', round(pipe.score(X_train_vec_rf, y_train_rf), 4))\n",
        "print('Test Accuracy: ', round(pipe.score(X_test_vec_rf, y_test_rf), 4))"
      ],
      "metadata": {
        "id": "BfARcuclvVQx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#build pipe with PCA \n",
        "pipe = Pipeline([\n",
        "                 ('pca', pca),\n",
        "                 ('rfc', rfc),\n",
        "                 ])\n",
        "\n",
        "# Fit model\n",
        "start = time.time()\n",
        "pipe.fit(X_train_vec_rf, y_train_rf)\n",
        "end = time.time()\n",
        "print('Time: ', round(end-start, 4))\n",
        "print('Train Accuracy: ', round(pipe.score(X_train_vec_rf, y_train_rf), 4))\n",
        "print('Test Accuracy: ', round(pipe.score(X_test_vec_rf, y_test_rf), 4))"
      ],
      "metadata": {
        "id": "RTI2CafMtD9x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#build pipe with PCA & StandardScaler\n",
        "\n",
        "pipe = Pipeline([\n",
        "                 ('scaler', scaler),\n",
        "                 ('pca', pca),\n",
        "                 ('rfc', rfc),\n",
        "                 ])\n",
        "\n",
        "# Fit model\n",
        "start = time.time()\n",
        "pipe.fit(X_train_vec_rf, y_train_rf)\n",
        "end = time.time()\n",
        "print('Time: ', round(end-start, 4))\n",
        "print('Train Accuracy: ', round(pipe.score(X_train_vec_rf, y_train_rf), 4))\n",
        "print('Test Accuracy: ', round(pipe.score(X_test_vec_rf, y_test_rf), 4))"
      ],
      "metadata": {
        "id": "WInkSWaAuI5X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Linear Support Vector Classification\n",
        "<h2> 6.1 LinearSVC without any data cleaning"
      ],
      "metadata": {
        "id": "rkwceP7z6AwD"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iKbIfED5-rGf"
      },
      "source": [
        "svc_data=pd.read_csv('https://raw.githubusercontent.com/chadi-aebi/DMML2021_Rolex/main/data/training_data.csv', index_col='id')\n",
        "svc_test_df = pd.read_csv('https://raw.githubusercontent.com/chadi-aebi/DMML2021_Rolex/main/data/unlabelled_test_data.csv', index_col='id')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2uCUzcV-rGi"
      },
      "source": [
        "X_svc = svc_data['sentence']\n",
        "ylabels_svc = svc_data['difficulty']\n",
        "\n",
        "X_train_svc, X_test_svc, y_train_svc, y_test_svc = train_test_split(X_svc, ylabels_svc, test_size=0.2, random_state=0, stratify=ylabels_svc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_svc_df = pd.DataFrame(X_train_svc)"
      ],
      "metadata": {
        "id": "RnKyzFO7-rGi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G60yfcj1-rGj"
      },
      "source": [
        "# Define classifier\n",
        "svc = LinearSVC()\n",
        "# svc = LinearSVC(random_state=1, dual=False, max_iter=10000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VE9V5CFB-rGk"
      },
      "source": [
        "# Create pipeline with tfidf\n",
        "pipe = Pipeline([('vectorizer', tfidf_vector),\n",
        "                 ('classifier', svc)])\n",
        "\n",
        "# Fit model on training set\n",
        "pipe.fit(X_train_svc, y_train_svc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_svc = pipe.predict(X_test_svc)\n",
        "\n",
        "acc_svc_0 = accuracy_score(y_test_svc,y_pred_svc)\n",
        "evaluate(y_test_svc, y_pred_svc)"
      ],
      "metadata": {
        "id": "U7zNG1eG-rGl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_conf_mat(y_test_svc, y_pred_svc)"
      ],
      "metadata": {
        "id": "Hes9HPfbh3o-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2> LinearSVC with Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "5t24-OMJAVNk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define parameters to test\n",
        "\n",
        "\n",
        "#grid_svc = [ {'C': [1, 10, 100, 1000], \n",
        "#              'penalty': ['l1', 'l2'], \n",
        "#              'loss': ['hinge','squared_hinge'], \n",
        "#             # 'dual' : ['false', 'true'], \n",
        "#              'max_iter' : [100, 1000, 10000]}]\n",
        "\n",
        "#second round\n",
        "#grid_svc = [ {\n",
        "            #'C': [1, 10, 100, 1000], \n",
        "            #  'penalty': ['l1', 'l2'], \n",
        "            #  'loss': ['hinge','squared_hinge'], \n",
        "             # 'dual' : ['false', 'true'], \n",
        "#              'max_iter' : [1, 100, 300, 500, 700, 900]}]\n",
        "\n",
        "# Define and fit model\n",
        "\n",
        "#svc = LogisticSVC()\n",
        "#svc_cv = GridSearchCV(svc, grid_svc, cv=10)\n",
        "#pipe = Pipeline([('vectorizer', tfidf_vector),\n",
        " #                ('classifier', svc_cv)])\n",
        "\n",
        "#pipe.fit(X_train_svc, y_train_svc)\n",
        "\n",
        "# Print results\n",
        "\n",
        "#print(\"Hyperparameters:\", svc_cv.best_params_)\n",
        "#y_pred_svc = pipe.predict(X_test_svc)\n",
        "#evaluate(y_test_svc, y_pred_svc)"
      ],
      "metadata": {
        "id": "bvv8kt1CA4WW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#safe best parameters\n",
        "#Hyperparameters= {'C': 1, 'loss': 'squared_hinge', 'max_iter': 100, 'penalty': 'l2'}"
      ],
      "metadata": {
        "id": "jJQr3-9-A4Wa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2> 6.3 LinearSVC with further methods\n",
        "<h3> 6.3.1 Preprocessing"
      ],
      "metadata": {
        "id": "dflHJzozBX0b"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0s1QMvaBmZZ"
      },
      "source": [
        "tfidf_vec_svc = TfidfVectorizer(tokenizer=spacy_tokenizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define classifier with best params - \n",
        "svc = LinearSVC(C = 1, loss = 'squared_hinge', max_iter= 100)"
      ],
      "metadata": {
        "id": "MuuLGLCzBmZa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create pipeline with tfidf \n",
        "pipe = Pipeline([('vectorizer', tfidf_vec_svc),\n",
        "                 ('classifier', svc)])\n",
        "\n",
        "# Fit model on training set\n",
        "pipe.fit(X_train_svc, y_train_svc)"
      ],
      "metadata": {
        "id": "9g14eTUeBmZc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UFtTLoTPBmZe"
      },
      "source": [
        "y_pred_svc = pipe.predict(X_test_svc)\n",
        "\n",
        "evaluate(y_test_svc, y_pred_svc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create pipeline with tfidf\n",
        "pipe = Pipeline([('vectorizer', tfidf_vec_svc),\n",
        "                 ('classifier', svc)])\n",
        "\n",
        "# Fit model on whole data set\n",
        "pipe.fit(X_svc, ylabels_svc)"
      ],
      "metadata": {
        "id": "sj7gkoQtBmZd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_test=pipe.predict(svc_test_df['sentence'])"
      ],
      "metadata": {
        "id": "u8L7L3BvJaHr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission_test_svc = pd.DataFrame(y_pred_test, columns=['difficulty'])\n",
        "#submission_test_svc"
      ],
      "metadata": {
        "id": "iGfS2UPiI_MB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission_test_svc.to_csv('submission_21-12-16.csv')"
      ],
      "metadata": {
        "id": "6MSaBm2HI_MD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3> 6.3.2 Word Embedding"
      ],
      "metadata": {
        "id": "m8t3ls46GObg"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uvNhL0a5GRoy"
      },
      "source": [
        "#Vectorizing - Word Embeddings\n",
        "\n",
        "with nlp.disable_pipes():\n",
        "    doc_vectors = np.array([nlp(sentence).vector for sentence in svc_data.sentence])\n",
        "    \n",
        "doc_vectors.shape\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train_svc, X_test_svc, y_train_svc, y_test_svc = train_test_split(doc_vectors, svc_data.difficulty,\n",
        "                                                    test_size=0.2, random_state=0)\n",
        "\n"
      ],
      "metadata": {
        "id": "0q8Q_zRnGRo1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit model on training set\n",
        "svc.fit(X_train_svc, y_train_svc)"
      ],
      "metadata": {
        "id": "7AiX-wt2GRo2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_svc = svc.predict(X_test_svc)\n",
        "\n",
        "evaluate(y_test_svc, y_pred_svc)"
      ],
      "metadata": {
        "id": "forpiHjLGRo3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3> 6.3.3 Dimensionality Reduction"
      ],
      "metadata": {
        "id": "mj9kEEmcGoxb"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8or-lex0HJOX"
      },
      "source": [
        "pca = PCA(n_components=3000) #n_components can be varied to try out different models"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VuUd5qYsHJOZ"
      },
      "source": [
        "X_svc = svc_data['sentence']\n",
        "ylabels_svc = svc_data['difficulty']\n",
        "\n",
        "X_train_svc, X_test_svc, y_train_svc, y_test_svc = train_test_split(X_svc, ylabels_svc, test_size=0.2, random_state=0, stratify=ylabels_svc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#X_train_vec_svc = tfidf_vector.fit_transform(X_train_svc).toarray()\n",
        "#X_test_vec_svc = tfidf_vector.transform(X_test_svc).toarray()\n",
        "#print(X_train_vec_svc.shape)"
      ],
      "metadata": {
        "id": "t2nZfLsFHJOc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_vec_svc = tfidf_vec_svc.fit_transform(X_train_svc).toarray()\n",
        "X_test_vec_svc = tfidf_vec_svc.transform(X_test_svc).toarray()\n",
        "print(X_train_vec_svc.shape)"
      ],
      "metadata": {
        "id": "qODPbm1ZKsM5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#build pipe without Scaler & PCA\n",
        "scaler = StandardScaler()\n",
        "pipe = Pipeline([\n",
        "                 ('svc', svc),\n",
        "                 ])\n",
        "\n",
        "# Fit model\n",
        "pipe.fit(X_train_vec_svc, y_train_svc)\n",
        "print('Train Accuracy: ', round(pipe.score(X_train_vec_svc, y_train_svc), 4))\n",
        "print('Test Accuracy: ', round(pipe.score(X_test_vec_svc, y_test_svc), 4))"
      ],
      "metadata": {
        "id": "7pq_8TVSHJOd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#build pipe with  StandardScaler\n",
        "scaler = StandardScaler()\n",
        "pipe = Pipeline([\n",
        "                 ('scaler', scaler),\n",
        "                 ('svc', svc),\n",
        "                 ])\n",
        "\n",
        "# Fit model\n",
        "pipe.fit(X_train_vec_svc, y_train_svc)\n",
        "print('Train Accuracy: ', round(pipe.score(X_train_vec_svc, y_train_svc), 4))\n",
        "print('Test Accuracy: ', round(pipe.score(X_test_vec_svc, y_test_svc), 4))"
      ],
      "metadata": {
        "id": "oHyVCXM9HJOe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#build pipe with PCA \n",
        "pipe = Pipeline([\n",
        "                 ('pca', pca),\n",
        "                 ('svc', svc),\n",
        "                 ])\n",
        "\n",
        "# Fit model\n",
        "pipe.fit(X_train_vec_svc, y_train_svc)\n",
        "print('Train Accuracy: ', round(pipe.score(X_train_vec_svc, y_train_svc), 4))\n",
        "print('Test Accuracy: ', round(pipe.score(X_test_vec_svc, y_test_svc), 4))"
      ],
      "metadata": {
        "id": "qlC7K1ESHJOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#build pipe with PCA & StandardScaler\n",
        "\n",
        "pipe = Pipeline([\n",
        "                 ('scaler', scaler),\n",
        "                 ('pca', pca),\n",
        "                 ('svc', svc),\n",
        "                 ])\n",
        "\n",
        "# Fit model\n",
        "pipe.fit(X_train_vec_svc, y_train_svc)\n",
        "print('Train Accuracy: ', round(pipe.score(X_train_vec_svc, y_train_svc), 4))\n",
        "print('Test Accuracy: ', round(pipe.score(X_test_vec_svc, y_test_svc), 4))"
      ],
      "metadata": {
        "id": "L70WxPxmHJOg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Submission with whole data set"
      ],
      "metadata": {
        "id": "Hm6QFmuO6vuS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_vec_svc = tfidf_vec_svc.fit_transform(X_svc).toarray()\n",
        "X_test_vec_svc = tfidf_vec_svc.transform(svc_test_df['sentence']).toarray()\n",
        "print(X_train_vec_svc.shape)"
      ],
      "metadata": {
        "id": "_m77ULEg6v8a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#build pipe with PCA \n",
        "pipe = Pipeline([\n",
        "                 ('pca', pca),\n",
        "                 ('svc', svc),\n",
        "                 ])\n",
        "\n",
        "# Fit model\n",
        "pipe.fit(X_train_vec_svc, ylabels_svc)"
      ],
      "metadata": {
        "id": "AiILr1Xy625d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_test=pipe.predict(X_test_vec_svc)"
      ],
      "metadata": {
        "id": "ZEh08A_I7CRu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission_test = pd.DataFrame(y_pred_test, columns=['difficulty'])\n",
        "#submission_test"
      ],
      "metadata": {
        "id": "pe3cHY7E7CRv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission_test.to_csv('submission_21-12-17_1.csv')"
      ],
      "metadata": {
        "id": "Iv3-h2rE7CRw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "tyPM1sld8ho1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}